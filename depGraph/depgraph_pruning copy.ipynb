{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL BEFORE PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Pruning with 20.0% percentage on 6 layers...\n",
      "Pruning layer: conv2\n",
      "Pruning layer: conv3\n",
      "Pruning layer: conv4\n",
      "Pruning layer: conv5\n",
      "Pruning layer: fc1\n",
      "Pruning layer: fc2\n",
      "MODEL AFTER PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 154, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(154, 308, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(308, 205, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(205, 205, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=7380, out_features=3277, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=3277, out_features=3277, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=3277, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_pruning as tp\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class AlexNetFineTuner(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetFineTuner, self).__init__()\n",
    "\n",
    "        # Load pre-trained AlexNet\n",
    "        self.model = models.alexnet(pretrained=False)\n",
    "        self.model.classifier[6] = nn.Linear(4096, num_classes)  # Update the classifier layer\n",
    "\n",
    "    def prune_model(self, pruning_percentage=0.2):\n",
    "        # Prepare example inputs for building the dependency graph\n",
    "        example_inputs = torch.randn(1, 3, 224, 224, dtype=torch.float32)\n",
    "\n",
    "        print(\"MODEL BEFORE PRUNING:\\n\", self.model)\n",
    "\n",
    "        # Build the Dependency Graph\n",
    "        DG = tp.DependencyGraph().build_dependency(self.model, example_inputs)\n",
    "\n",
    "        # Layers to prune\n",
    "        layers_to_prune = {\n",
    "            \"conv2\": self.model.features[3],    # second conv layer in AlexNet\n",
    "            \"conv3\": self.model.features[6],\n",
    "            \"conv4\": self.model.features[8],\n",
    "            \"conv5\": self.model.features[10],\n",
    "            \"fc1\": self.model.classifier[1],\n",
    "            \"fc2\": self.model.classifier[4]\n",
    "        }\n",
    "\n",
    "        # Helper function to compute pruning indices based on magnitude\n",
    "        def get_pruning_indices(module, percentage):\n",
    "            with torch.no_grad():\n",
    "                weight = module.weight.data\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    channel_norms = weight.abs().mean(dim=[1, 2, 3])  # Compute L1 norm of each output channel\n",
    "                elif isinstance(module, nn.Linear):\n",
    "                    channel_norms = weight.abs().mean(dim=1)  # Compute L1 norm of each output feature\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "                pruning_count = int(channel_norms.size(0) * percentage)\n",
    "                if pruning_count == 0:\n",
    "                    return []  # No pruning if percentage is too small\n",
    "\n",
    "                _, prune_indices = torch.topk(channel_norms, pruning_count, largest=False)\n",
    "                return prune_indices.tolist()\n",
    "\n",
    "        # Collect pruning groups for all layers\n",
    "        groups = []\n",
    "        for layer_name, layer_module in layers_to_prune.items():\n",
    "            if isinstance(layer_module, nn.Conv2d):\n",
    "                prune_fn = tp.prune_conv_out_channels\n",
    "            elif isinstance(layer_module, nn.Linear):\n",
    "                prune_fn = tp.prune_linear_out_channels\n",
    "            else:\n",
    "                print(f\"Skipping {layer_name}: Unsupported layer type {type(layer_module)}\")\n",
    "                continue\n",
    "\n",
    "            pruning_idxs = get_pruning_indices(layer_module, pruning_percentage)\n",
    "            if pruning_idxs is None or len(pruning_idxs) == 0:\n",
    "                print(f\"No channels to prune for {layer_name}.\")\n",
    "                continue\n",
    "\n",
    "            group = DG.get_pruning_group(layer_module, prune_fn, idxs=pruning_idxs)\n",
    "            if DG.check_pruning_group(group):\n",
    "                groups.append((layer_name, group))\n",
    "            else:\n",
    "                print(f\"Invalid pruning group for layer {layer_name}, skipping pruning.\")\n",
    "\n",
    "        # Prune all layers together if we have at least one valid group\n",
    "        if groups:\n",
    "            print(f\"Pruning with {pruning_percentage*100}% percentage on {len(groups)} layers...\")\n",
    "            for layer_name, group in groups:\n",
    "                print(f\"Pruning layer: {layer_name}\")\n",
    "                group.prune()\n",
    "\n",
    "            print(\"MODEL AFTER PRUNING:\\n\", self.model)\n",
    "        else:\n",
    "            print(\"No valid pruning groups found. The model was not pruned.\")\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    model = AlexNetFineTuner()\n",
    "    model.prune_model(pruning_percentage=0.2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
