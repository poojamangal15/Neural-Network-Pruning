{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torch_pruning as tp\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ColorJitter, RandomCrop\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torchmetrics.functional as tmf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetFineTuner(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4, num_classes=10):\n",
    "        super(AlexNetFineTuner, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load pre-trained AlexNet\n",
    "        self.model = models.alexnet(pretrained=False)\n",
    "        self.model.classifier[6] = torch.nn.Linear(4096, num_classes)  # Update the classifier layer\n",
    "\n",
    "        # Initialize metrics dictionary\n",
    "        self.metrics = {\n",
    "            \"pruning_percentage\": [],\n",
    "            \"test_accuracy\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"model_size\": []\n",
    "        }\n",
    "        \n",
    "        self.test_outputs = []\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def print_dependency_graph(self, DG):\n",
    "        print(\"\\nDependency Graph Details:\")\n",
    "        for module, node in DG.module2node.items():\n",
    "            print(f\"Module: {module}\")\n",
    "            for dep in node.dependencies:\n",
    "                print(f\"    * Target Module: {dep.target.module}\")\n",
    "\n",
    "    def visualize_dependency_graph(self, DG):\n",
    "        \"\"\"Visualize the dependency graph using networkx.\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        for module, node in DG.module2node.items():\n",
    "            for dep in node.dependencies:\n",
    "                G.add_edge(str(module), str(dep.target.module))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, with_labels=True, node_size=1000, font_size=8, node_color=\"skyblue\", edge_color=\"gray\")\n",
    "        plt.title(\"Dependency Graph\")\n",
    "        plt.show()\n",
    "\n",
    "    def prune_model(self, pruning_percentage=0.2, train_loader=None, val_loader=None, fine_tune_epochs=5, fine_tune_lr=1e-5):\n",
    "        # Prepare example inputs for building the dependency graph\n",
    "        example_inputs = torch.randn(1, 3, 224, 224, dtype=torch.float32, device=\"mps\")\n",
    "\n",
    "        # Ensure model is on the correct device and dtype\n",
    "        self.model = self.model.to(\"mps\").to(torch.float32)\n",
    "\n",
    "        print(\"MODEL BEFORE PRUNING:\\n\", self.model)\n",
    "\n",
    "        # Build the Dependency Graph\n",
    "        DG = tp.DependencyGraph().build_dependency(self.model, example_inputs)\n",
    "\n",
    "        # Layers to prune: you can adjust this as needed\n",
    "        layers_to_prune = {\n",
    "            # Skip the first layer: model.features[0]\n",
    "            \"conv2\": self.model.features[3],    # second conv layer in AlexNet\n",
    "            \"conv3\": self.model.features[6],\n",
    "            \"conv4\": self.model.features[8],\n",
    "            \"conv5\": self.model.features[10],\n",
    "\n",
    "            # Skip the last layer: model.classifier[6]\n",
    "            \"fc1\": self.model.classifier[1],\n",
    "            \"fc2\": self.model.classifier[4]\n",
    "            # Do not prune fc3 (model.classifier[6]) as it is the final output layer\n",
    "        }\n",
    "\n",
    "        # Helper function to compute pruning indices based on magnitude\n",
    "        def get_pruning_indices(module, percentage):\n",
    "            with torch.no_grad():\n",
    "                weight = module.weight.data\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    # Compute L1 norm of each output channel\n",
    "                    channel_norms = weight.abs().mean(dim=[1,2,3])  # shape: [out_channels]\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    channel_norms = weight.abs().mean(dim=1)         # shape: [out_features]\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "                pruning_count = int(channel_norms.size(0) * percentage)\n",
    "                if pruning_count == 0:\n",
    "                    return []  # No pruning if percentage is too small\n",
    "\n",
    "                # Sort indices by magnitude (ascending order)\n",
    "                _, prune_indices = torch.topk(channel_norms, pruning_count, largest=False)\n",
    "                return prune_indices.tolist()\n",
    "\n",
    "        # Collect pruning groups for all layers\n",
    "        groups = []\n",
    "        for layer_name, layer_module in layers_to_prune.items():\n",
    "            if isinstance(layer_module, torch.nn.Conv2d):\n",
    "                prune_fn = tp.prune_conv_out_channels\n",
    "                num_features = layer_module.out_channels\n",
    "            elif isinstance(layer_module, torch.nn.Linear):\n",
    "                prune_fn = tp.prune_linear_out_channels\n",
    "                num_features = layer_module.out_features\n",
    "            else:\n",
    "                print(f\"Skipping {layer_name}: Unsupported layer type {type(layer_module)}\")\n",
    "                continue\n",
    "\n",
    "            # Get pruning indices based on magnitude\n",
    "            pruning_idxs = get_pruning_indices(layer_module, pruning_percentage)\n",
    "            if pruning_idxs is None or len(pruning_idxs) == 0:\n",
    "                # No pruning indices determined, skip this layer\n",
    "                print(f\"No channels to prune for {layer_name}.\")\n",
    "                continue\n",
    "\n",
    "            # Build the pruning group\n",
    "            group = DG.get_pruning_group(layer_module, prune_fn, idxs=pruning_idxs)\n",
    "            # Check if group is valid\n",
    "            if DG.check_pruning_group(group):\n",
    "                groups.append((layer_name, group))\n",
    "            else:\n",
    "                print(f\"Invalid pruning group for layer {layer_name}, skipping pruning.\")\n",
    "\n",
    "        # Prune all layers together if we have at least one valid group\n",
    "        if groups:\n",
    "            print(f\"Pruning with {pruning_percentage*100}% percentage on {len(groups)} layers...\")\n",
    "            for layer_name, group in groups:\n",
    "                print(f\"Pruning layer: {layer_name}\")\n",
    "                group.prune()\n",
    "\n",
    "            print(\"MODEL AFTER PRUNING:\\n\", self.model)\n",
    "        else:\n",
    "            print(\"No valid pruning groups found. The model was not pruned.\")\n",
    "            return\n",
    "\n",
    "        # Update metrics after pruning\n",
    "        pruned_num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.metrics[\"model_size\"].append(pruned_num_params)\n",
    "        self.metrics[\"pruning_percentage\"].append(pruning_percentage * 100)\n",
    "\n",
    "        # Optional fine-tuning after pruning\n",
    "        if train_loader is not None and val_loader is not None:\n",
    "            print(\"Starting fine-tuning of the pruned model...\")\n",
    "            self.fine_tune_model(self.model, train_loader, val_loader, fine_tune_epochs, fine_tune_lr)\n",
    "        else:\n",
    "            print(\"No train/val loaders provided for fine-tuning.\")\n",
    "\n",
    "        # # Perform testing after pruning\n",
    "        # if hasattr(self, \"trainer\"):\n",
    "        #     print(\"Testing the pruned model...\")\n",
    "        #     self.trainer.test(self)\n",
    "        # else:\n",
    "        #     print(\"Trainer is not set. Testing skipped.\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)  # Convert inputs to float32\n",
    "        labels = labels.to(\"mps\")  # Ensure labels are on MPS\n",
    "\n",
    "        print(\"Images device and dtype:---------------------\", images.device, images.dtype)\n",
    "        print(\"Labels device and dtype:\", labels.device, labels.dtype)\n",
    "\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)  # Convert inputs to float32\n",
    "        labels = labels.to(\"mps\")  # Ensure labels are on MPS\n",
    "        outputs = self(images)\n",
    "        val_loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_acc = (preds == labels).float().mean()\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)\n",
    "        labels = labels.to(\"mps\")\n",
    "        outputs = self(images)\n",
    "        test_loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_acc = (preds == labels).float().mean()\n",
    "\n",
    "        print(f\"Test Step: Batch {batch_idx}, Loss: {test_loss.item()}, Acc: {test_acc.item()}\")\n",
    "\n",
    "        # Store batch results for aggregation later\n",
    "        self.test_outputs.append({\"test_loss\": test_loss.item(), \"test_acc\": test_acc.item()})\n",
    "\n",
    "        # Log per-batch results if needed\n",
    "        self.log(\"test_loss_batch\", test_loss, prog_bar=True)\n",
    "        self.log(\"test_acc_batch\", test_acc, prog_bar=True)\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Aggregate metrics across batches\n",
    "        avg_loss = sum(o[\"test_loss\"] for o in self.test_outputs) / len(self.test_outputs)\n",
    "        avg_acc = sum(o[\"test_acc\"] for o in self.test_outputs) / len(self.test_outputs)\n",
    "\n",
    "        print(f\"Test Epoch End: Avg Loss: {avg_loss}, Avg Acc: {avg_acc}\")\n",
    "\n",
    "        # Append aggregated metrics for the epoch\n",
    "        self.metrics[\"test_accuracy\"].append(avg_acc)\n",
    "        self.metrics[\"test_loss\"].append(avg_loss)\n",
    "\n",
    "        # Log aggregated metrics\n",
    "        self.log(\"test_loss_epoch\", avg_loss, prog_bar=True)\n",
    "        self.log(\"test_acc_epoch\", avg_acc, prog_bar=True)\n",
    "\n",
    "        # Clear outputs for the next test epoch\n",
    "        self.test_outputs = []\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}\n",
    "    \n",
    "    def fine_tune_model(self, model, train_loader, val_loader, epochs=3\n",
    "    , learning_rate=1e-5):\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss, train_correct = 0, 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(\"mps\"), labels.to(\"mps\")\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(\"mps\"), labels.to(\"mps\")\n",
    "                    outputs = model(images)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Compute metrics\n",
    "            train_acc = train_correct / len(train_loader.dataset)\n",
    "            val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Instead of self.log, just print or store the metrics\n",
    "            print(f\"fine_tune_train_loss: {train_loss}, fine_tune_train_acc: {train_acc}, \"\n",
    "            f\"fine_tune_val_loss: {val_loss}, fine_tune_val_acc: {val_acc}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the given dataloader and returns accuracy and F1 score.\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                images = images.to(\"mps\").to(torch.float32)\n",
    "                labels = labels.to(\"mps\")\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')  # Weighted for class imbalance\n",
    "\n",
    "        return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    print(\"Metrics Debug:\")\n",
    "    print(\"Pruning Percentages:\", metrics[\"pruning_percentage\"])\n",
    "    print(\"Test Accuracy:\", metrics[\"test_accuracy\"])\n",
    "    print(\"Test Loss:\", metrics[\"test_loss\"])\n",
    "    print(\"Model Size:\", metrics[\"model_size\"])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"test_accuracy\"], marker='o', label=\"Accuracy\")\n",
    "    plt.title(\"Test Accuracy vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"test_loss\"], marker='o', color=\"orange\", label=\"Loss\")\n",
    "    plt.title(\"Test Loss vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"model_size\"], marker='o', color=\"green\", label=\"Model Size\")\n",
    "    plt.title(\"Model Size vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Number of Parameters\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9sztqcfd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>test_acc_batch</td><td>▁</td></tr><tr><td>test_acc_epoch</td><td>▁</td></tr><tr><td>test_loss_batch</td><td>▁</td></tr><tr><td>test_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>test_acc_batch</td><td>0.9104</td></tr><tr><td>test_acc_epoch</td><td>0.91034</td></tr><tr><td>test_loss_batch</td><td>0.27592</td></tr><tr><td>test_loss_epoch</td><td>0.27594</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_Run</strong> at: <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/9sztqcfd' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/9sztqcfd</a><br/> View project at: <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241208_191807-9sztqcfd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9sztqcfd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/poojamangal/Desktop/Masters/NNProject/depGraph/wandb/run-20241208_200938-i1sbpiwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/i1sbpiwk' target=\"_blank\">AlexNet_Run</a></strong> to <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/i1sbpiwk' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/i1sbpiwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Applying 10.0% pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 0.8985, Original F1 Score: 0.8983\n",
      "MODEL BEFORE PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Pruning with 10.0% percentage on 6 layers...\n",
      "Pruning layer: conv2\n",
      "Pruning layer: conv3\n",
      "Pruning layer: conv4\n",
      "Pruning layer: conv5\n",
      "Pruning layer: fc1\n",
      "Pruning layer: fc2\n",
      "MODEL AFTER PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 173, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(173, 346, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(346, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(231, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=8316, out_features=3687, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=3687, out_features=3687, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=3687, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting fine-tuning of the pruned model...\n",
      "Epoch 1/5, Train Loss: 385.3723, Train Acc: 0.8939, Val Loss: 68.9889, Val Acc: 0.9258\n",
      "fine_tune_train_loss: 385.3722598552704, fine_tune_train_acc: 0.893875, fine_tune_val_loss: 68.98891719430685, fine_tune_val_acc: 0.9258\n",
      "Epoch 2/5, Train Loss: 301.6845, Train Acc: 0.9164, Val Loss: 68.3591, Val Acc: 0.9264\n",
      "fine_tune_train_loss: 301.6844700239599, fine_tune_train_acc: 0.916375, fine_tune_val_loss: 68.3591150790453, fine_tune_val_acc: 0.9264\n",
      "Epoch 3/5, Train Loss: 252.4269, Train Acc: 0.9299, Val Loss: 64.9905, Val Acc: 0.9272\n",
      "fine_tune_train_loss: 252.42689883708954, fine_tune_train_acc: 0.9299, fine_tune_val_loss: 64.99053280986845, fine_tune_val_acc: 0.9272\n",
      "Epoch 4/5, Train Loss: 214.6718, Train Acc: 0.9402, Val Loss: 61.4099, Val Acc: 0.9329\n",
      "fine_tune_train_loss: 214.6718103196472, fine_tune_train_acc: 0.9402, fine_tune_val_loss: 61.40986588597298, fine_tune_val_acc: 0.9329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 177.8018, Train Acc: 0.9503, Val Loss: 63.4116, Val Acc: 0.9303\n",
      "fine_tune_train_loss: 177.8018131442368, fine_tune_train_acc: 0.950325, fine_tune_val_loss: 63.4115900285542, fine_tune_val_acc: 0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68437e33a31a4345b52dd9a86ced0c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step: Batch 0, Loss: 0.14631585776805878, Acc: 0.9375\n",
      "Test Step: Batch 1, Loss: 0.2824646234512329, Acc: 0.84375\n",
      "Test Step: Batch 2, Loss: 0.11372699588537216, Acc: 1.0\n",
      "Test Step: Batch 3, Loss: 0.16452844440937042, Acc: 0.96875\n",
      "Test Step: Batch 4, Loss: 0.38436663150787354, Acc: 0.84375\n",
      "Test Step: Batch 5, Loss: 0.21165843307971954, Acc: 0.875\n",
      "Test Step: Batch 6, Loss: 0.23700092732906342, Acc: 0.9375\n",
      "Test Step: Batch 7, Loss: 0.27812695503234863, Acc: 0.9375\n",
      "Test Step: Batch 8, Loss: 0.2716137766838074, Acc: 0.90625\n",
      "Test Step: Batch 9, Loss: 0.30974727869033813, Acc: 0.90625\n",
      "Test Step: Batch 10, Loss: 0.11985360085964203, Acc: 0.96875\n",
      "Test Step: Batch 11, Loss: 0.3579644560813904, Acc: 0.875\n",
      "Test Step: Batch 12, Loss: 0.39265888929367065, Acc: 0.90625\n",
      "Test Step: Batch 13, Loss: 0.1786435842514038, Acc: 0.9375\n",
      "Test Step: Batch 14, Loss: 0.3099755644798279, Acc: 0.90625\n",
      "Test Step: Batch 15, Loss: 0.05974894389510155, Acc: 1.0\n",
      "Test Step: Batch 16, Loss: 0.265468567609787, Acc: 0.875\n",
      "Test Step: Batch 17, Loss: 0.061486195772886276, Acc: 1.0\n",
      "Test Step: Batch 18, Loss: 0.10084755718708038, Acc: 0.96875\n",
      "Test Step: Batch 19, Loss: 0.19129730761051178, Acc: 0.90625\n",
      "Test Step: Batch 20, Loss: 0.5146369934082031, Acc: 0.875\n",
      "Test Step: Batch 21, Loss: 0.2573634386062622, Acc: 0.875\n",
      "Test Step: Batch 22, Loss: 0.4190570116043091, Acc: 0.8125\n",
      "Test Step: Batch 23, Loss: 0.28947532176971436, Acc: 0.9375\n",
      "Test Step: Batch 24, Loss: 0.2142535150051117, Acc: 0.90625\n",
      "Test Step: Batch 25, Loss: 0.18053650856018066, Acc: 0.9375\n",
      "Test Step: Batch 26, Loss: 0.3001401424407959, Acc: 0.90625\n",
      "Test Step: Batch 27, Loss: 0.3648073673248291, Acc: 0.90625\n",
      "Test Step: Batch 28, Loss: 0.2086477428674698, Acc: 0.90625\n",
      "Test Step: Batch 29, Loss: 0.06559007614850998, Acc: 0.96875\n",
      "Test Step: Batch 30, Loss: 0.12057389318943024, Acc: 0.9375\n",
      "Test Step: Batch 31, Loss: 0.2481437772512436, Acc: 0.90625\n",
      "Test Step: Batch 32, Loss: 0.4993729889392853, Acc: 0.875\n",
      "Test Step: Batch 33, Loss: 0.2541002631187439, Acc: 0.9375\n",
      "Test Step: Batch 34, Loss: 0.2143269181251526, Acc: 0.90625\n",
      "Test Step: Batch 35, Loss: 0.5300770998001099, Acc: 0.8125\n",
      "Test Step: Batch 36, Loss: 0.3984416127204895, Acc: 0.875\n",
      "Test Step: Batch 37, Loss: 0.04267745092511177, Acc: 0.96875\n",
      "Test Step: Batch 38, Loss: 0.3881816864013672, Acc: 0.84375\n",
      "Test Step: Batch 39, Loss: 0.19009718298912048, Acc: 0.9375\n",
      "Test Step: Batch 40, Loss: 0.05136585235595703, Acc: 1.0\n",
      "Test Step: Batch 41, Loss: 0.4421849548816681, Acc: 0.9375\n",
      "Test Step: Batch 42, Loss: 0.07627886533737183, Acc: 0.96875\n",
      "Test Step: Batch 43, Loss: 0.17217667400836945, Acc: 0.9375\n",
      "Test Step: Batch 44, Loss: 0.30321598052978516, Acc: 0.90625\n",
      "Test Step: Batch 45, Loss: 0.23842978477478027, Acc: 0.90625\n",
      "Test Step: Batch 46, Loss: 0.546226441860199, Acc: 0.78125\n",
      "Test Step: Batch 47, Loss: 0.3210006356239319, Acc: 0.875\n",
      "Test Step: Batch 48, Loss: 0.15051952004432678, Acc: 0.96875\n",
      "Test Step: Batch 49, Loss: 0.26740556955337524, Acc: 0.90625\n",
      "Test Step: Batch 50, Loss: 0.2692681849002838, Acc: 0.9375\n",
      "Test Step: Batch 51, Loss: 0.11790256202220917, Acc: 0.9375\n",
      "Test Step: Batch 52, Loss: 0.4874880909919739, Acc: 0.90625\n",
      "Test Step: Batch 53, Loss: 0.5003127455711365, Acc: 0.84375\n",
      "Test Step: Batch 54, Loss: 0.5012991428375244, Acc: 0.84375\n",
      "Test Step: Batch 55, Loss: 0.530307412147522, Acc: 0.84375\n",
      "Test Step: Batch 56, Loss: 0.25006216764450073, Acc: 0.90625\n",
      "Test Step: Batch 57, Loss: 0.12392690032720566, Acc: 0.9375\n",
      "Test Step: Batch 58, Loss: 0.2437610924243927, Acc: 0.9375\n",
      "Test Step: Batch 59, Loss: 0.2041032314300537, Acc: 0.96875\n",
      "Test Step: Batch 60, Loss: 0.48550212383270264, Acc: 0.875\n",
      "Test Step: Batch 61, Loss: 0.23261034488677979, Acc: 0.96875\n",
      "Test Step: Batch 62, Loss: 0.30919143557548523, Acc: 0.84375\n",
      "Test Step: Batch 63, Loss: 0.20123398303985596, Acc: 0.9375\n",
      "Test Step: Batch 64, Loss: 0.23239636421203613, Acc: 0.875\n",
      "Test Step: Batch 65, Loss: 0.1934693157672882, Acc: 0.90625\n",
      "Test Step: Batch 66, Loss: 0.40185096859931946, Acc: 0.90625\n",
      "Test Step: Batch 67, Loss: 0.4544609785079956, Acc: 0.875\n",
      "Test Step: Batch 68, Loss: 0.24624593555927277, Acc: 0.90625\n",
      "Test Step: Batch 69, Loss: 0.4820481240749359, Acc: 0.9375\n",
      "Test Step: Batch 70, Loss: 0.6727909445762634, Acc: 0.78125\n",
      "Test Step: Batch 71, Loss: 0.40157049894332886, Acc: 0.84375\n",
      "Test Step: Batch 72, Loss: 0.5081626772880554, Acc: 0.90625\n",
      "Test Step: Batch 73, Loss: 0.2383813112974167, Acc: 0.9375\n",
      "Test Step: Batch 74, Loss: 0.22303993999958038, Acc: 0.9375\n",
      "Test Step: Batch 75, Loss: 0.6476733684539795, Acc: 0.90625\n",
      "Test Step: Batch 76, Loss: 0.2800307869911194, Acc: 0.875\n",
      "Test Step: Batch 77, Loss: 0.5304766893386841, Acc: 0.90625\n",
      "Test Step: Batch 78, Loss: 0.5181437730789185, Acc: 0.875\n",
      "Test Step: Batch 79, Loss: 0.5220777988433838, Acc: 0.90625\n",
      "Test Step: Batch 80, Loss: 0.35079145431518555, Acc: 0.875\n",
      "Test Step: Batch 81, Loss: 0.6171771883964539, Acc: 0.84375\n",
      "Test Step: Batch 82, Loss: 0.1316361129283905, Acc: 0.9375\n",
      "Test Step: Batch 83, Loss: 0.08359144628047943, Acc: 0.96875\n",
      "Test Step: Batch 84, Loss: 0.24062322080135345, Acc: 0.90625\n",
      "Test Step: Batch 85, Loss: 0.09117159992456436, Acc: 0.96875\n",
      "Test Step: Batch 86, Loss: 0.26017096638679504, Acc: 0.90625\n",
      "Test Step: Batch 87, Loss: 0.2618665099143982, Acc: 0.9375\n",
      "Test Step: Batch 88, Loss: 0.5841334462165833, Acc: 0.8125\n",
      "Test Step: Batch 89, Loss: 0.17257052659988403, Acc: 0.90625\n",
      "Test Step: Batch 90, Loss: 0.0695846676826477, Acc: 0.96875\n",
      "Test Step: Batch 91, Loss: 0.131134033203125, Acc: 0.96875\n",
      "Test Step: Batch 92, Loss: 0.3954079747200012, Acc: 0.875\n",
      "Test Step: Batch 93, Loss: 0.11674294620752335, Acc: 0.96875\n",
      "Test Step: Batch 94, Loss: 0.16175544261932373, Acc: 0.90625\n",
      "Test Step: Batch 95, Loss: 0.38043540716171265, Acc: 0.90625\n",
      "Test Step: Batch 96, Loss: 0.3206213712692261, Acc: 0.90625\n",
      "Test Step: Batch 97, Loss: 0.2693352699279785, Acc: 0.90625\n",
      "Test Step: Batch 98, Loss: 0.2214900106191635, Acc: 0.90625\n",
      "Test Step: Batch 99, Loss: 0.36349618434906006, Acc: 0.875\n",
      "Test Step: Batch 100, Loss: 0.47513657808303833, Acc: 0.78125\n",
      "Test Step: Batch 101, Loss: 0.06786322593688965, Acc: 1.0\n",
      "Test Step: Batch 102, Loss: 0.06218353658914566, Acc: 1.0\n",
      "Test Step: Batch 103, Loss: 0.6353858113288879, Acc: 0.8125\n",
      "Test Step: Batch 104, Loss: 0.3010218143463135, Acc: 0.875\n",
      "Test Step: Batch 105, Loss: 0.1582963466644287, Acc: 0.90625\n",
      "Test Step: Batch 106, Loss: 0.30649808049201965, Acc: 0.875\n",
      "Test Step: Batch 107, Loss: 0.5433073043823242, Acc: 0.90625\n",
      "Test Step: Batch 108, Loss: 0.13622143864631653, Acc: 0.96875\n",
      "Test Step: Batch 109, Loss: 0.2484787106513977, Acc: 0.90625\n",
      "Test Step: Batch 110, Loss: 0.19724418222904205, Acc: 0.9375\n",
      "Test Step: Batch 111, Loss: 0.19249385595321655, Acc: 0.90625\n",
      "Test Step: Batch 112, Loss: 0.7395005226135254, Acc: 0.8125\n",
      "Test Step: Batch 113, Loss: 0.3113176226615906, Acc: 0.84375\n",
      "Test Step: Batch 114, Loss: 0.19021372497081757, Acc: 0.90625\n",
      "Test Step: Batch 115, Loss: 0.1925705373287201, Acc: 0.90625\n",
      "Test Step: Batch 116, Loss: 0.1723279505968094, Acc: 0.9375\n",
      "Test Step: Batch 117, Loss: 0.30040472745895386, Acc: 0.9375\n",
      "Test Step: Batch 118, Loss: 0.49992695450782776, Acc: 0.90625\n",
      "Test Step: Batch 119, Loss: 0.6140543222427368, Acc: 0.875\n",
      "Test Step: Batch 120, Loss: 0.14569306373596191, Acc: 0.96875\n",
      "Test Step: Batch 121, Loss: 0.24436825513839722, Acc: 0.96875\n",
      "Test Step: Batch 122, Loss: 0.24588000774383545, Acc: 0.90625\n",
      "Test Step: Batch 123, Loss: 0.43064969778060913, Acc: 0.90625\n",
      "Test Step: Batch 124, Loss: 0.3233271837234497, Acc: 0.9375\n",
      "Test Step: Batch 125, Loss: 0.3256649374961853, Acc: 0.875\n",
      "Test Step: Batch 126, Loss: 0.39666882157325745, Acc: 0.8125\n",
      "Test Step: Batch 127, Loss: 0.0943174958229065, Acc: 0.96875\n",
      "Test Step: Batch 128, Loss: 0.10525863617658615, Acc: 0.96875\n",
      "Test Step: Batch 129, Loss: 0.3558875322341919, Acc: 0.875\n",
      "Test Step: Batch 130, Loss: 0.2021333873271942, Acc: 0.96875\n",
      "Test Step: Batch 131, Loss: 0.5466363430023193, Acc: 0.875\n",
      "Test Step: Batch 132, Loss: 0.1257856786251068, Acc: 0.9375\n",
      "Test Step: Batch 133, Loss: 0.18481384217739105, Acc: 0.96875\n",
      "Test Step: Batch 134, Loss: 0.3157460689544678, Acc: 0.875\n",
      "Test Step: Batch 135, Loss: 0.16713815927505493, Acc: 0.96875\n",
      "Test Step: Batch 136, Loss: 0.25533124804496765, Acc: 0.875\n",
      "Test Step: Batch 137, Loss: 0.30728620290756226, Acc: 0.90625\n",
      "Test Step: Batch 138, Loss: 0.1842479556798935, Acc: 0.9375\n",
      "Test Step: Batch 139, Loss: 0.1363266408443451, Acc: 0.96875\n",
      "Test Step: Batch 140, Loss: 0.17277149856090546, Acc: 0.90625\n",
      "Test Step: Batch 141, Loss: 0.316435307264328, Acc: 0.84375\n",
      "Test Step: Batch 142, Loss: 0.2765694558620453, Acc: 0.84375\n",
      "Test Step: Batch 143, Loss: 0.2327669858932495, Acc: 0.90625\n",
      "Test Step: Batch 144, Loss: 0.5478681325912476, Acc: 0.8125\n",
      "Test Step: Batch 145, Loss: 0.13678403198719025, Acc: 0.9375\n",
      "Test Step: Batch 146, Loss: 0.1574908196926117, Acc: 0.9375\n",
      "Test Step: Batch 147, Loss: 0.30643463134765625, Acc: 0.90625\n",
      "Test Step: Batch 148, Loss: 0.5852271914482117, Acc: 0.875\n",
      "Test Step: Batch 149, Loss: 0.562213659286499, Acc: 0.875\n",
      "Test Step: Batch 150, Loss: 0.09389779716730118, Acc: 0.96875\n",
      "Test Step: Batch 151, Loss: 0.0709964781999588, Acc: 0.96875\n",
      "Test Step: Batch 152, Loss: 0.31275323033332825, Acc: 0.875\n",
      "Test Step: Batch 153, Loss: 0.4269293546676636, Acc: 0.875\n",
      "Test Step: Batch 154, Loss: 0.4235117435455322, Acc: 0.90625\n",
      "Test Step: Batch 155, Loss: 0.44077610969543457, Acc: 0.84375\n",
      "Test Step: Batch 156, Loss: 0.4102526903152466, Acc: 0.8125\n",
      "Test Step: Batch 157, Loss: 0.09626699239015579, Acc: 0.96875\n",
      "Test Step: Batch 158, Loss: 0.23739275336265564, Acc: 0.90625\n",
      "Test Step: Batch 159, Loss: 0.07846707105636597, Acc: 0.96875\n",
      "Test Step: Batch 160, Loss: 0.14740882813930511, Acc: 0.9375\n",
      "Test Step: Batch 161, Loss: 0.2163737714290619, Acc: 0.875\n",
      "Test Step: Batch 162, Loss: 0.31947705149650574, Acc: 0.8125\n",
      "Test Step: Batch 163, Loss: 0.4633222818374634, Acc: 0.90625\n",
      "Test Step: Batch 164, Loss: 0.0981486514210701, Acc: 0.96875\n",
      "Test Step: Batch 165, Loss: 0.259591281414032, Acc: 0.9375\n",
      "Test Step: Batch 166, Loss: 0.19910860061645508, Acc: 0.96875\n",
      "Test Step: Batch 167, Loss: 0.13837462663650513, Acc: 0.9375\n",
      "Test Step: Batch 168, Loss: 0.2487787902355194, Acc: 0.9375\n",
      "Test Step: Batch 169, Loss: 0.3299441933631897, Acc: 0.84375\n",
      "Test Step: Batch 170, Loss: 0.27236491441726685, Acc: 0.875\n",
      "Test Step: Batch 171, Loss: 0.0804583728313446, Acc: 0.96875\n",
      "Test Step: Batch 172, Loss: 0.5590997934341431, Acc: 0.90625\n",
      "Test Step: Batch 173, Loss: 0.3979783058166504, Acc: 0.90625\n",
      "Test Step: Batch 174, Loss: 0.3326355814933777, Acc: 0.90625\n",
      "Test Step: Batch 175, Loss: 0.1517573595046997, Acc: 0.96875\n",
      "Test Step: Batch 176, Loss: 0.40594157576560974, Acc: 0.8125\n",
      "Test Step: Batch 177, Loss: 0.07124139368534088, Acc: 0.96875\n",
      "Test Step: Batch 178, Loss: 0.10045253485441208, Acc: 1.0\n",
      "Test Step: Batch 179, Loss: 0.5426411628723145, Acc: 0.8125\n",
      "Test Step: Batch 180, Loss: 0.11104011535644531, Acc: 0.9375\n",
      "Test Step: Batch 181, Loss: 0.38012075424194336, Acc: 0.875\n",
      "Test Step: Batch 182, Loss: 0.21864114701747894, Acc: 0.90625\n",
      "Test Step: Batch 183, Loss: 0.4446830749511719, Acc: 0.875\n",
      "Test Step: Batch 184, Loss: 0.37921491265296936, Acc: 0.90625\n",
      "Test Step: Batch 185, Loss: 0.10335112363100052, Acc: 0.96875\n",
      "Test Step: Batch 186, Loss: 0.1525343954563141, Acc: 0.96875\n",
      "Test Step: Batch 187, Loss: 0.35273003578186035, Acc: 0.90625\n",
      "Test Step: Batch 188, Loss: 0.21126563847064972, Acc: 0.9375\n",
      "Test Step: Batch 189, Loss: 0.47070571780204773, Acc: 0.84375\n",
      "Test Step: Batch 190, Loss: 0.07765359431505203, Acc: 0.96875\n",
      "Test Step: Batch 191, Loss: 0.13409987092018127, Acc: 0.90625\n",
      "Test Step: Batch 192, Loss: 0.24983972311019897, Acc: 0.875\n",
      "Test Step: Batch 193, Loss: 0.24648407101631165, Acc: 0.875\n",
      "Test Step: Batch 194, Loss: 0.17527048289775848, Acc: 0.9375\n",
      "Test Step: Batch 195, Loss: 0.376910924911499, Acc: 0.90625\n",
      "Test Step: Batch 196, Loss: 0.049829065799713135, Acc: 1.0\n",
      "Test Step: Batch 197, Loss: 0.07247374206781387, Acc: 0.96875\n",
      "Test Step: Batch 198, Loss: 0.060795411467552185, Acc: 0.96875\n",
      "Test Step: Batch 199, Loss: 0.21717408299446106, Acc: 0.875\n",
      "Test Step: Batch 200, Loss: 0.3582261800765991, Acc: 0.84375\n",
      "Test Step: Batch 201, Loss: 0.1435459703207016, Acc: 0.90625\n",
      "Test Step: Batch 202, Loss: 0.1935555338859558, Acc: 0.9375\n",
      "Test Step: Batch 203, Loss: 0.14912107586860657, Acc: 0.96875\n",
      "Test Step: Batch 204, Loss: 0.08544989675283432, Acc: 0.96875\n",
      "Test Step: Batch 205, Loss: 0.24136272072792053, Acc: 0.875\n",
      "Test Step: Batch 206, Loss: 0.06113243103027344, Acc: 0.96875\n",
      "Test Step: Batch 207, Loss: 0.20670340955257416, Acc: 0.875\n",
      "Test Step: Batch 208, Loss: 0.3936730623245239, Acc: 0.90625\n",
      "Test Step: Batch 209, Loss: 0.056127678602933884, Acc: 1.0\n",
      "Test Step: Batch 210, Loss: 0.32282114028930664, Acc: 0.9375\n",
      "Test Step: Batch 211, Loss: 0.486175000667572, Acc: 0.8125\n",
      "Test Step: Batch 212, Loss: 0.26091423630714417, Acc: 0.9375\n",
      "Test Step: Batch 213, Loss: 0.17450670897960663, Acc: 0.9375\n",
      "Test Step: Batch 214, Loss: 0.39453938603401184, Acc: 0.8125\n",
      "Test Step: Batch 215, Loss: 0.20436179637908936, Acc: 0.96875\n",
      "Test Step: Batch 216, Loss: 0.20431207120418549, Acc: 0.90625\n",
      "Test Step: Batch 217, Loss: 0.27367183566093445, Acc: 0.84375\n",
      "Test Step: Batch 218, Loss: 0.43760672211647034, Acc: 0.84375\n",
      "Test Step: Batch 219, Loss: 0.131064772605896, Acc: 0.9375\n",
      "Test Step: Batch 220, Loss: 0.5024480819702148, Acc: 0.9375\n",
      "Test Step: Batch 221, Loss: 0.2448272854089737, Acc: 0.90625\n",
      "Test Step: Batch 222, Loss: 0.3964727818965912, Acc: 0.90625\n",
      "Test Step: Batch 223, Loss: 0.19947215914726257, Acc: 0.96875\n",
      "Test Step: Batch 224, Loss: 0.2975691258907318, Acc: 0.90625\n",
      "Test Step: Batch 225, Loss: 0.37963050603866577, Acc: 0.8125\n",
      "Test Step: Batch 226, Loss: 0.1858217418193817, Acc: 0.875\n",
      "Test Step: Batch 227, Loss: 0.153864786028862, Acc: 0.96875\n",
      "Test Step: Batch 228, Loss: 0.1798926591873169, Acc: 0.9375\n",
      "Test Step: Batch 229, Loss: 0.07307490706443787, Acc: 0.96875\n",
      "Test Step: Batch 230, Loss: 0.17989815771579742, Acc: 0.90625\n",
      "Test Step: Batch 231, Loss: 0.5967845916748047, Acc: 0.78125\n",
      "Test Step: Batch 232, Loss: 0.4390273690223694, Acc: 0.875\n",
      "Test Step: Batch 233, Loss: 0.008435753174126148, Acc: 1.0\n",
      "Test Step: Batch 234, Loss: 0.21897318959236145, Acc: 0.9375\n",
      "Test Step: Batch 235, Loss: 0.06340745091438293, Acc: 0.96875\n",
      "Test Step: Batch 236, Loss: 0.26441287994384766, Acc: 0.9375\n",
      "Test Step: Batch 237, Loss: 0.18209396302700043, Acc: 0.9375\n",
      "Test Step: Batch 238, Loss: 0.23413027822971344, Acc: 0.90625\n",
      "Test Step: Batch 239, Loss: 0.30645325779914856, Acc: 0.90625\n",
      "Test Step: Batch 240, Loss: 0.26389747858047485, Acc: 0.90625\n",
      "Test Step: Batch 241, Loss: 0.10934139788150787, Acc: 0.9375\n",
      "Test Step: Batch 242, Loss: 0.6285392045974731, Acc: 0.84375\n",
      "Test Step: Batch 243, Loss: 0.381888210773468, Acc: 0.84375\n",
      "Test Step: Batch 244, Loss: 0.506490170955658, Acc: 0.875\n",
      "Test Step: Batch 245, Loss: 0.25047436356544495, Acc: 0.90625\n",
      "Test Step: Batch 246, Loss: 0.34302160143852234, Acc: 0.90625\n",
      "Test Step: Batch 247, Loss: 0.2755824029445648, Acc: 0.9375\n",
      "Test Step: Batch 248, Loss: 0.3228541612625122, Acc: 0.875\n",
      "Test Step: Batch 249, Loss: 0.09762731939554214, Acc: 0.9375\n",
      "Test Step: Batch 250, Loss: 0.21816417574882507, Acc: 0.90625\n",
      "Test Step: Batch 251, Loss: 0.3968779444694519, Acc: 0.875\n",
      "Test Step: Batch 252, Loss: 0.18411202728748322, Acc: 0.90625\n",
      "Test Step: Batch 253, Loss: 0.3659661114215851, Acc: 0.90625\n",
      "Test Step: Batch 254, Loss: 0.08798360824584961, Acc: 0.96875\n",
      "Test Step: Batch 255, Loss: 0.28945645689964294, Acc: 0.9375\n",
      "Test Step: Batch 256, Loss: 0.22400817275047302, Acc: 0.90625\n",
      "Test Step: Batch 257, Loss: 0.14489686489105225, Acc: 0.9375\n",
      "Test Step: Batch 258, Loss: 0.3152897357940674, Acc: 0.875\n",
      "Test Step: Batch 259, Loss: 0.25371283292770386, Acc: 0.90625\n",
      "Test Step: Batch 260, Loss: 0.2361169457435608, Acc: 0.96875\n",
      "Test Step: Batch 261, Loss: 0.12126089632511139, Acc: 0.9375\n",
      "Test Step: Batch 262, Loss: 0.35007932782173157, Acc: 0.90625\n",
      "Test Step: Batch 263, Loss: 0.20609048008918762, Acc: 0.90625\n",
      "Test Step: Batch 264, Loss: 0.5683900713920593, Acc: 0.875\n",
      "Test Step: Batch 265, Loss: 0.1392582356929779, Acc: 0.9375\n",
      "Test Step: Batch 266, Loss: 0.4187760353088379, Acc: 0.875\n",
      "Test Step: Batch 267, Loss: 0.08915320038795471, Acc: 1.0\n",
      "Test Step: Batch 268, Loss: 0.3141576647758484, Acc: 0.90625\n",
      "Test Step: Batch 269, Loss: 0.0975191593170166, Acc: 1.0\n",
      "Test Step: Batch 270, Loss: 0.3276154100894928, Acc: 0.875\n",
      "Test Step: Batch 271, Loss: 0.17242848873138428, Acc: 0.9375\n",
      "Test Step: Batch 272, Loss: 0.3000456988811493, Acc: 0.9375\n",
      "Test Step: Batch 273, Loss: 0.5263200998306274, Acc: 0.90625\n",
      "Test Step: Batch 274, Loss: 0.2777538299560547, Acc: 0.875\n",
      "Test Step: Batch 275, Loss: 0.3099484443664551, Acc: 0.8125\n",
      "Test Step: Batch 276, Loss: 0.13169756531715393, Acc: 0.96875\n",
      "Test Step: Batch 277, Loss: 0.1313326507806778, Acc: 0.9375\n",
      "Test Step: Batch 278, Loss: 0.0975072905421257, Acc: 0.96875\n",
      "Test Step: Batch 279, Loss: 0.15033039450645447, Acc: 0.96875\n",
      "Test Step: Batch 280, Loss: 0.19779323041439056, Acc: 0.9375\n",
      "Test Step: Batch 281, Loss: 0.10879667103290558, Acc: 0.96875\n",
      "Test Step: Batch 282, Loss: 0.5725126266479492, Acc: 0.84375\n",
      "Test Step: Batch 283, Loss: 0.27342915534973145, Acc: 0.90625\n",
      "Test Step: Batch 284, Loss: 0.11206886917352676, Acc: 0.9375\n",
      "Test Step: Batch 285, Loss: 0.42429235577583313, Acc: 0.875\n",
      "Test Step: Batch 286, Loss: 0.016222313046455383, Acc: 1.0\n",
      "Test Step: Batch 287, Loss: 0.06625429540872574, Acc: 0.96875\n",
      "Test Step: Batch 288, Loss: 0.24342858791351318, Acc: 0.875\n",
      "Test Step: Batch 289, Loss: 0.18535973131656647, Acc: 0.90625\n",
      "Test Step: Batch 290, Loss: 0.4121316373348236, Acc: 0.875\n",
      "Test Step: Batch 291, Loss: 0.06313563883304596, Acc: 1.0\n",
      "Test Step: Batch 292, Loss: 0.31324365735054016, Acc: 0.84375\n",
      "Test Step: Batch 293, Loss: 0.3332694172859192, Acc: 0.875\n",
      "Test Step: Batch 294, Loss: 0.34268680214881897, Acc: 0.90625\n",
      "Test Step: Batch 295, Loss: 0.1891002655029297, Acc: 0.9375\n",
      "Test Step: Batch 296, Loss: 0.08175860345363617, Acc: 0.9375\n",
      "Test Step: Batch 297, Loss: 0.7241083383560181, Acc: 0.84375\n",
      "Test Step: Batch 298, Loss: 0.14849594235420227, Acc: 0.9375\n",
      "Test Step: Batch 299, Loss: 0.2836408019065857, Acc: 0.90625\n",
      "Test Step: Batch 300, Loss: 0.10135418176651001, Acc: 0.9375\n",
      "Test Step: Batch 301, Loss: 0.4924129247665405, Acc: 0.9375\n",
      "Test Step: Batch 302, Loss: 0.20543277263641357, Acc: 0.9375\n",
      "Test Step: Batch 303, Loss: 0.31345120072364807, Acc: 0.9375\n",
      "Test Step: Batch 304, Loss: 0.4324303865432739, Acc: 0.84375\n",
      "Test Step: Batch 305, Loss: 0.3129822015762329, Acc: 0.90625\n",
      "Test Step: Batch 306, Loss: 0.5293360948562622, Acc: 0.8125\n",
      "Test Step: Batch 307, Loss: 0.4518894553184509, Acc: 0.875\n",
      "Test Step: Batch 308, Loss: 0.2381526380777359, Acc: 0.90625\n",
      "Test Step: Batch 309, Loss: 0.2886684536933899, Acc: 0.90625\n",
      "Test Step: Batch 310, Loss: 0.13369818031787872, Acc: 0.96875\n",
      "Test Step: Batch 311, Loss: 0.45660728216171265, Acc: 0.78125\n",
      "Test Step: Batch 312, Loss: 0.2795538604259491, Acc: 0.9375\n",
      "Test Epoch End: Avg Loss: 0.2745140034842272, Avg Acc: 0.9108426517571885\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9107999801635742     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9108426570892334     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2745059132575989     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.27451398968696594    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9107999801635742    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9108426570892334    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2745059132575989    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.27451398968696594   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Accuracy: 0.9108, Pruned F1 Score: 0.9105\n",
      "[(0.8985, 0.9108)]\n",
      "[(0.8982989644479569, 0.9104842430179791)]\n",
      "Pruned model saved to: ./pruned_models/alexnet_pruned_10.pth\n",
      "Applying 30.0% pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 0.8985, Original F1 Score: 0.8983\n",
      "MODEL BEFORE PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Pruning with 30.0% percentage on 6 layers...\n",
      "Pruning layer: conv2\n",
      "Pruning layer: conv3\n",
      "Pruning layer: conv4\n",
      "Pruning layer: conv5\n",
      "Pruning layer: fc1\n",
      "Pruning layer: fc2\n",
      "MODEL AFTER PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 135, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(135, 269, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(269, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=6480, out_features=2868, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=2868, out_features=2868, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=2868, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting fine-tuning of the pruned model...\n",
      "Epoch 1/5, Train Loss: 773.3030, Train Acc: 0.7924, Val Loss: 125.4114, Val Acc: 0.8640\n",
      "fine_tune_train_loss: 773.302980452776, fine_tune_train_acc: 0.7924, fine_tune_val_loss: 125.41140978038311, fine_tune_val_acc: 0.864\n",
      "Epoch 2/5, Train Loss: 521.0646, Train Acc: 0.8562, Val Loss: 110.4146, Val Acc: 0.8795\n",
      "fine_tune_train_loss: 521.064555555582, fine_tune_train_acc: 0.856175, fine_tune_val_loss: 110.41463827341795, fine_tune_val_acc: 0.8795\n",
      "Epoch 3/5, Train Loss: 445.8866, Train Acc: 0.8751, Val Loss: 100.6293, Val Acc: 0.8910\n",
      "fine_tune_train_loss: 445.886646322906, fine_tune_train_acc: 0.87515, fine_tune_val_loss: 100.62934762239456, fine_tune_val_acc: 0.891\n",
      "Epoch 4/5, Train Loss: 390.0523, Train Acc: 0.8914, Val Loss: 95.3714, Val Acc: 0.8967\n",
      "fine_tune_train_loss: 390.05231599882245, fine_tune_train_acc: 0.8914, fine_tune_val_loss: 95.37142736464739, fine_tune_val_acc: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 346.6209, Train Acc: 0.9038, Val Loss: 92.3870, Val Acc: 0.8979\n",
      "fine_tune_train_loss: 346.6208808682859, fine_tune_train_acc: 0.9038, fine_tune_val_loss: 92.38703694194555, fine_tune_val_acc: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d914ae43e904051a8c23a66b9d81ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step: Batch 0, Loss: 0.14066149294376373, Acc: 0.9375\n",
      "Test Step: Batch 1, Loss: 0.36464929580688477, Acc: 0.78125\n",
      "Test Step: Batch 2, Loss: 0.16011108458042145, Acc: 0.9375\n",
      "Test Step: Batch 3, Loss: 0.17304259538650513, Acc: 0.9375\n",
      "Test Step: Batch 4, Loss: 0.48332101106643677, Acc: 0.78125\n",
      "Test Step: Batch 5, Loss: 0.35300517082214355, Acc: 0.875\n",
      "Test Step: Batch 6, Loss: 0.2761918306350708, Acc: 0.9375\n",
      "Test Step: Batch 7, Loss: 0.44289517402648926, Acc: 0.8125\n",
      "Test Step: Batch 8, Loss: 0.40754806995391846, Acc: 0.84375\n",
      "Test Step: Batch 9, Loss: 0.25256744027137756, Acc: 0.875\n",
      "Test Step: Batch 10, Loss: 0.12387935817241669, Acc: 0.9375\n",
      "Test Step: Batch 11, Loss: 0.5061367154121399, Acc: 0.78125\n",
      "Test Step: Batch 12, Loss: 0.41144609451293945, Acc: 0.90625\n",
      "Test Step: Batch 13, Loss: 0.26848018169403076, Acc: 0.90625\n",
      "Test Step: Batch 14, Loss: 0.3619556128978729, Acc: 0.90625\n",
      "Test Step: Batch 15, Loss: 0.15282577276229858, Acc: 0.90625\n",
      "Test Step: Batch 16, Loss: 0.3002997636795044, Acc: 0.875\n",
      "Test Step: Batch 17, Loss: 0.08206024765968323, Acc: 1.0\n",
      "Test Step: Batch 18, Loss: 0.11595644056797028, Acc: 0.96875\n",
      "Test Step: Batch 19, Loss: 0.1466202437877655, Acc: 0.90625\n",
      "Test Step: Batch 20, Loss: 0.5142824649810791, Acc: 0.875\n",
      "Test Step: Batch 21, Loss: 0.4228251874446869, Acc: 0.84375\n",
      "Test Step: Batch 22, Loss: 0.4464167654514313, Acc: 0.875\n",
      "Test Step: Batch 23, Loss: 0.2518145442008972, Acc: 0.875\n",
      "Test Step: Batch 24, Loss: 0.4536258578300476, Acc: 0.8125\n",
      "Test Step: Batch 25, Loss: 0.18249329924583435, Acc: 0.90625\n",
      "Test Step: Batch 26, Loss: 0.4430742859840393, Acc: 0.84375\n",
      "Test Step: Batch 27, Loss: 0.45495209097862244, Acc: 0.90625\n",
      "Test Step: Batch 28, Loss: 0.31067609786987305, Acc: 0.875\n",
      "Test Step: Batch 29, Loss: 0.295705646276474, Acc: 0.9375\n",
      "Test Step: Batch 30, Loss: 0.23423539102077484, Acc: 0.9375\n",
      "Test Step: Batch 31, Loss: 0.37933045625686646, Acc: 0.90625\n",
      "Test Step: Batch 32, Loss: 0.575839638710022, Acc: 0.8125\n",
      "Test Step: Batch 33, Loss: 0.38633835315704346, Acc: 0.90625\n",
      "Test Step: Batch 34, Loss: 0.2684042155742645, Acc: 0.875\n",
      "Test Step: Batch 35, Loss: 0.6024901866912842, Acc: 0.8125\n",
      "Test Step: Batch 36, Loss: 0.4441453516483307, Acc: 0.84375\n",
      "Test Step: Batch 37, Loss: 0.15236948430538177, Acc: 0.9375\n",
      "Test Step: Batch 38, Loss: 0.518263578414917, Acc: 0.75\n",
      "Test Step: Batch 39, Loss: 0.3020673990249634, Acc: 0.90625\n",
      "Test Step: Batch 40, Loss: 0.18829724192619324, Acc: 0.84375\n",
      "Test Step: Batch 41, Loss: 0.21766486763954163, Acc: 0.9375\n",
      "Test Step: Batch 42, Loss: 0.1576160192489624, Acc: 0.90625\n",
      "Test Step: Batch 43, Loss: 0.4004690945148468, Acc: 0.84375\n",
      "Test Step: Batch 44, Loss: 0.104187972843647, Acc: 0.96875\n",
      "Test Step: Batch 45, Loss: 0.28782880306243896, Acc: 0.90625\n",
      "Test Step: Batch 46, Loss: 0.5674881339073181, Acc: 0.8125\n",
      "Test Step: Batch 47, Loss: 0.22810019552707672, Acc: 0.90625\n",
      "Test Step: Batch 48, Loss: 0.20180080831050873, Acc: 0.96875\n",
      "Test Step: Batch 49, Loss: 0.5237520933151245, Acc: 0.84375\n",
      "Test Step: Batch 50, Loss: 0.27793705463409424, Acc: 0.875\n",
      "Test Step: Batch 51, Loss: 0.17346462607383728, Acc: 0.96875\n",
      "Test Step: Batch 52, Loss: 0.5629527568817139, Acc: 0.84375\n",
      "Test Step: Batch 53, Loss: 0.4092938005924225, Acc: 0.84375\n",
      "Test Step: Batch 54, Loss: 0.42128580808639526, Acc: 0.84375\n",
      "Test Step: Batch 55, Loss: 0.4422534108161926, Acc: 0.875\n",
      "Test Step: Batch 56, Loss: 0.3006817102432251, Acc: 0.875\n",
      "Test Step: Batch 57, Loss: 0.15476860105991364, Acc: 0.9375\n",
      "Test Step: Batch 58, Loss: 0.5012511014938354, Acc: 0.78125\n",
      "Test Step: Batch 59, Loss: 0.3154173791408539, Acc: 0.9375\n",
      "Test Step: Batch 60, Loss: 0.4988871216773987, Acc: 0.84375\n",
      "Test Step: Batch 61, Loss: 0.2918291389942169, Acc: 0.90625\n",
      "Test Step: Batch 62, Loss: 0.3442123234272003, Acc: 0.875\n",
      "Test Step: Batch 63, Loss: 0.32210272550582886, Acc: 0.875\n",
      "Test Step: Batch 64, Loss: 0.27010783553123474, Acc: 0.90625\n",
      "Test Step: Batch 65, Loss: 0.37074583768844604, Acc: 0.8125\n",
      "Test Step: Batch 66, Loss: 0.49329325556755066, Acc: 0.875\n",
      "Test Step: Batch 67, Loss: 0.5014634132385254, Acc: 0.84375\n",
      "Test Step: Batch 68, Loss: 0.2385847568511963, Acc: 0.875\n",
      "Test Step: Batch 69, Loss: 0.32632917165756226, Acc: 0.9375\n",
      "Test Step: Batch 70, Loss: 0.5343159437179565, Acc: 0.75\n",
      "Test Step: Batch 71, Loss: 0.4378388822078705, Acc: 0.84375\n",
      "Test Step: Batch 72, Loss: 0.4823489189147949, Acc: 0.90625\n",
      "Test Step: Batch 73, Loss: 0.2505033016204834, Acc: 0.90625\n",
      "Test Step: Batch 74, Loss: 0.3733006715774536, Acc: 0.90625\n",
      "Test Step: Batch 75, Loss: 0.568631649017334, Acc: 0.875\n",
      "Test Step: Batch 76, Loss: 0.40608158707618713, Acc: 0.875\n",
      "Test Step: Batch 77, Loss: 0.5778674483299255, Acc: 0.84375\n",
      "Test Step: Batch 78, Loss: 0.6416650414466858, Acc: 0.8125\n",
      "Test Step: Batch 79, Loss: 0.555884063243866, Acc: 0.875\n",
      "Test Step: Batch 80, Loss: 0.31266725063323975, Acc: 0.875\n",
      "Test Step: Batch 81, Loss: 0.5407472848892212, Acc: 0.8125\n",
      "Test Step: Batch 82, Loss: 0.1637299507856369, Acc: 0.96875\n",
      "Test Step: Batch 83, Loss: 0.12424249202013016, Acc: 0.9375\n",
      "Test Step: Batch 84, Loss: 0.33412206172943115, Acc: 0.875\n",
      "Test Step: Batch 85, Loss: 0.22102423012256622, Acc: 0.9375\n",
      "Test Step: Batch 86, Loss: 0.45895683765411377, Acc: 0.875\n",
      "Test Step: Batch 87, Loss: 0.2934185266494751, Acc: 0.90625\n",
      "Test Step: Batch 88, Loss: 0.44601643085479736, Acc: 0.90625\n",
      "Test Step: Batch 89, Loss: 0.2463393360376358, Acc: 0.9375\n",
      "Test Step: Batch 90, Loss: 0.1445966362953186, Acc: 0.96875\n",
      "Test Step: Batch 91, Loss: 0.0774037316441536, Acc: 1.0\n",
      "Test Step: Batch 92, Loss: 0.4610666036605835, Acc: 0.875\n",
      "Test Step: Batch 93, Loss: 0.3620549440383911, Acc: 0.875\n",
      "Test Step: Batch 94, Loss: 0.21509520709514618, Acc: 0.9375\n",
      "Test Step: Batch 95, Loss: 0.5359731912612915, Acc: 0.84375\n",
      "Test Step: Batch 96, Loss: 0.2675401568412781, Acc: 0.875\n",
      "Test Step: Batch 97, Loss: 0.23615381121635437, Acc: 0.9375\n",
      "Test Step: Batch 98, Loss: 0.25145357847213745, Acc: 0.90625\n",
      "Test Step: Batch 99, Loss: 0.4353102743625641, Acc: 0.90625\n",
      "Test Step: Batch 100, Loss: 0.48853546380996704, Acc: 0.78125\n",
      "Test Step: Batch 101, Loss: 0.22960494458675385, Acc: 0.9375\n",
      "Test Step: Batch 102, Loss: 0.10846275091171265, Acc: 0.96875\n",
      "Test Step: Batch 103, Loss: 0.4611828327178955, Acc: 0.78125\n",
      "Test Step: Batch 104, Loss: 0.508323073387146, Acc: 0.8125\n",
      "Test Step: Batch 105, Loss: 0.21166740357875824, Acc: 0.9375\n",
      "Test Step: Batch 106, Loss: 0.3423598110675812, Acc: 0.875\n",
      "Test Step: Batch 107, Loss: 0.5138400793075562, Acc: 0.875\n",
      "Test Step: Batch 108, Loss: 0.18771864473819733, Acc: 0.96875\n",
      "Test Step: Batch 109, Loss: 0.35317036509513855, Acc: 0.90625\n",
      "Test Step: Batch 110, Loss: 0.4456024765968323, Acc: 0.9375\n",
      "Test Step: Batch 111, Loss: 0.2684687376022339, Acc: 0.90625\n",
      "Test Step: Batch 112, Loss: 0.5777783393859863, Acc: 0.8125\n",
      "Test Step: Batch 113, Loss: 0.23899909853935242, Acc: 0.84375\n",
      "Test Step: Batch 114, Loss: 0.22577975690364838, Acc: 0.875\n",
      "Test Step: Batch 115, Loss: 0.24262160062789917, Acc: 0.90625\n",
      "Test Step: Batch 116, Loss: 0.2751334607601166, Acc: 0.90625\n",
      "Test Step: Batch 117, Loss: 0.33114248514175415, Acc: 0.875\n",
      "Test Step: Batch 118, Loss: 0.513055682182312, Acc: 0.8125\n",
      "Test Step: Batch 119, Loss: 0.46886202692985535, Acc: 0.875\n",
      "Test Step: Batch 120, Loss: 0.26415547728538513, Acc: 0.90625\n",
      "Test Step: Batch 121, Loss: 0.18350261449813843, Acc: 0.96875\n",
      "Test Step: Batch 122, Loss: 0.33924874663352966, Acc: 0.875\n",
      "Test Step: Batch 123, Loss: 0.35745275020599365, Acc: 0.875\n",
      "Test Step: Batch 124, Loss: 0.3854716420173645, Acc: 0.90625\n",
      "Test Step: Batch 125, Loss: 0.2790037989616394, Acc: 0.84375\n",
      "Test Step: Batch 126, Loss: 0.38551080226898193, Acc: 0.875\n",
      "Test Step: Batch 127, Loss: 0.13960000872612, Acc: 0.96875\n",
      "Test Step: Batch 128, Loss: 0.24903427064418793, Acc: 0.90625\n",
      "Test Step: Batch 129, Loss: 0.4530845582485199, Acc: 0.875\n",
      "Test Step: Batch 130, Loss: 0.27307114005088806, Acc: 0.90625\n",
      "Test Step: Batch 131, Loss: 0.31479957699775696, Acc: 0.90625\n",
      "Test Step: Batch 132, Loss: 0.2893740236759186, Acc: 0.9375\n",
      "Test Step: Batch 133, Loss: 0.3001849055290222, Acc: 0.90625\n",
      "Test Step: Batch 134, Loss: 0.5812134742736816, Acc: 0.875\n",
      "Test Step: Batch 135, Loss: 0.1460999846458435, Acc: 0.96875\n",
      "Test Step: Batch 136, Loss: 0.19738370180130005, Acc: 0.90625\n",
      "Test Step: Batch 137, Loss: 0.45630741119384766, Acc: 0.875\n",
      "Test Step: Batch 138, Loss: 0.15660226345062256, Acc: 0.96875\n",
      "Test Step: Batch 139, Loss: 0.2430838644504547, Acc: 0.96875\n",
      "Test Step: Batch 140, Loss: 0.17497900128364563, Acc: 0.9375\n",
      "Test Step: Batch 141, Loss: 0.2799099087715149, Acc: 0.84375\n",
      "Test Step: Batch 142, Loss: 0.3775431215763092, Acc: 0.84375\n",
      "Test Step: Batch 143, Loss: 0.2895069420337677, Acc: 0.875\n",
      "Test Step: Batch 144, Loss: 0.6987912654876709, Acc: 0.6875\n",
      "Test Step: Batch 145, Loss: 0.17638272047042847, Acc: 0.9375\n",
      "Test Step: Batch 146, Loss: 0.25230881571769714, Acc: 0.90625\n",
      "Test Step: Batch 147, Loss: 0.31606361269950867, Acc: 0.90625\n",
      "Test Step: Batch 148, Loss: 0.6341201066970825, Acc: 0.78125\n",
      "Test Step: Batch 149, Loss: 0.5911872386932373, Acc: 0.8125\n",
      "Test Step: Batch 150, Loss: 0.14190134406089783, Acc: 0.9375\n",
      "Test Step: Batch 151, Loss: 0.1431349664926529, Acc: 0.9375\n",
      "Test Step: Batch 152, Loss: 0.46822619438171387, Acc: 0.875\n",
      "Test Step: Batch 153, Loss: 0.43919873237609863, Acc: 0.875\n",
      "Test Step: Batch 154, Loss: 0.3787146806716919, Acc: 0.90625\n",
      "Test Step: Batch 155, Loss: 0.4631670415401459, Acc: 0.84375\n",
      "Test Step: Batch 156, Loss: 0.3727562129497528, Acc: 0.8125\n",
      "Test Step: Batch 157, Loss: 0.1233915239572525, Acc: 0.9375\n",
      "Test Step: Batch 158, Loss: 0.2330080270767212, Acc: 0.875\n",
      "Test Step: Batch 159, Loss: 0.1736437976360321, Acc: 0.96875\n",
      "Test Step: Batch 160, Loss: 0.27345699071884155, Acc: 0.9375\n",
      "Test Step: Batch 161, Loss: 0.23782412707805634, Acc: 0.9375\n",
      "Test Step: Batch 162, Loss: 0.4571559727191925, Acc: 0.8125\n",
      "Test Step: Batch 163, Loss: 0.4246313273906708, Acc: 0.875\n",
      "Test Step: Batch 164, Loss: 0.20435872673988342, Acc: 0.9375\n",
      "Test Step: Batch 165, Loss: 0.2648608088493347, Acc: 0.84375\n",
      "Test Step: Batch 166, Loss: 0.2518867254257202, Acc: 0.90625\n",
      "Test Step: Batch 167, Loss: 0.23810747265815735, Acc: 0.90625\n",
      "Test Step: Batch 168, Loss: 0.22144325077533722, Acc: 0.90625\n",
      "Test Step: Batch 169, Loss: 0.36272376775741577, Acc: 0.875\n",
      "Test Step: Batch 170, Loss: 0.3934139311313629, Acc: 0.84375\n",
      "Test Step: Batch 171, Loss: 0.12923505902290344, Acc: 0.9375\n",
      "Test Step: Batch 172, Loss: 0.4748506546020508, Acc: 0.875\n",
      "Test Step: Batch 173, Loss: 0.4144996404647827, Acc: 0.84375\n",
      "Test Step: Batch 174, Loss: 0.42502033710479736, Acc: 0.84375\n",
      "Test Step: Batch 175, Loss: 0.27785664796829224, Acc: 0.84375\n",
      "Test Step: Batch 176, Loss: 0.5281529426574707, Acc: 0.78125\n",
      "Test Step: Batch 177, Loss: 0.12136609107255936, Acc: 0.96875\n",
      "Test Step: Batch 178, Loss: 0.16099366545677185, Acc: 0.96875\n",
      "Test Step: Batch 179, Loss: 0.3769201636314392, Acc: 0.875\n",
      "Test Step: Batch 180, Loss: 0.16225549578666687, Acc: 0.9375\n",
      "Test Step: Batch 181, Loss: 0.44294077157974243, Acc: 0.8125\n",
      "Test Step: Batch 182, Loss: 0.2879631221294403, Acc: 0.90625\n",
      "Test Step: Batch 183, Loss: 0.4174480438232422, Acc: 0.78125\n",
      "Test Step: Batch 184, Loss: 0.45522791147232056, Acc: 0.78125\n",
      "Test Step: Batch 185, Loss: 0.172159343957901, Acc: 0.9375\n",
      "Test Step: Batch 186, Loss: 0.1707417517900467, Acc: 0.9375\n",
      "Test Step: Batch 187, Loss: 0.4865835905075073, Acc: 0.84375\n",
      "Test Step: Batch 188, Loss: 0.28713202476501465, Acc: 0.90625\n",
      "Test Step: Batch 189, Loss: 0.6478592157363892, Acc: 0.71875\n",
      "Test Step: Batch 190, Loss: 0.10056941956281662, Acc: 1.0\n",
      "Test Step: Batch 191, Loss: 0.3097531199455261, Acc: 0.90625\n",
      "Test Step: Batch 192, Loss: 0.3971955180168152, Acc: 0.84375\n",
      "Test Step: Batch 193, Loss: 0.24516664445400238, Acc: 0.9375\n",
      "Test Step: Batch 194, Loss: 0.25285959243774414, Acc: 0.875\n",
      "Test Step: Batch 195, Loss: 0.35128289461135864, Acc: 0.90625\n",
      "Test Step: Batch 196, Loss: 0.12359374761581421, Acc: 1.0\n",
      "Test Step: Batch 197, Loss: 0.149061918258667, Acc: 0.9375\n",
      "Test Step: Batch 198, Loss: 0.11775840818881989, Acc: 0.9375\n",
      "Test Step: Batch 199, Loss: 0.26968854665756226, Acc: 0.875\n",
      "Test Step: Batch 200, Loss: 0.4540518522262573, Acc: 0.8125\n",
      "Test Step: Batch 201, Loss: 0.30103999376296997, Acc: 0.90625\n",
      "Test Step: Batch 202, Loss: 0.27829641103744507, Acc: 0.875\n",
      "Test Step: Batch 203, Loss: 0.2296868860721588, Acc: 0.90625\n",
      "Test Step: Batch 204, Loss: 0.10709500312805176, Acc: 1.0\n",
      "Test Step: Batch 205, Loss: 0.23457762598991394, Acc: 0.90625\n",
      "Test Step: Batch 206, Loss: 0.09725239872932434, Acc: 0.96875\n",
      "Test Step: Batch 207, Loss: 0.3522343635559082, Acc: 0.90625\n",
      "Test Step: Batch 208, Loss: 0.4592877924442291, Acc: 0.90625\n",
      "Test Step: Batch 209, Loss: 0.13768965005874634, Acc: 0.96875\n",
      "Test Step: Batch 210, Loss: 0.38857701420783997, Acc: 0.875\n",
      "Test Step: Batch 211, Loss: 0.5844812393188477, Acc: 0.78125\n",
      "Test Step: Batch 212, Loss: 0.2680458724498749, Acc: 0.9375\n",
      "Test Step: Batch 213, Loss: 0.21018102765083313, Acc: 0.9375\n",
      "Test Step: Batch 214, Loss: 0.5732837319374084, Acc: 0.75\n",
      "Test Step: Batch 215, Loss: 0.26298004388809204, Acc: 0.90625\n",
      "Test Step: Batch 216, Loss: 0.3780261278152466, Acc: 0.875\n",
      "Test Step: Batch 217, Loss: 0.35124891996383667, Acc: 0.875\n",
      "Test Step: Batch 218, Loss: 0.7727227807044983, Acc: 0.78125\n",
      "Test Step: Batch 219, Loss: 0.16504058241844177, Acc: 0.90625\n",
      "Test Step: Batch 220, Loss: 0.4632853865623474, Acc: 0.90625\n",
      "Test Step: Batch 221, Loss: 0.16011205315589905, Acc: 0.96875\n",
      "Test Step: Batch 222, Loss: 0.29695892333984375, Acc: 0.90625\n",
      "Test Step: Batch 223, Loss: 0.16531355679035187, Acc: 0.90625\n",
      "Test Step: Batch 224, Loss: 0.32681357860565186, Acc: 0.9375\n",
      "Test Step: Batch 225, Loss: 0.3729941248893738, Acc: 0.84375\n",
      "Test Step: Batch 226, Loss: 0.22047963738441467, Acc: 0.90625\n",
      "Test Step: Batch 227, Loss: 0.2551700174808502, Acc: 0.9375\n",
      "Test Step: Batch 228, Loss: 0.2955839931964874, Acc: 0.90625\n",
      "Test Step: Batch 229, Loss: 0.1713179647922516, Acc: 0.96875\n",
      "Test Step: Batch 230, Loss: 0.19319584965705872, Acc: 0.9375\n",
      "Test Step: Batch 231, Loss: 0.5851764678955078, Acc: 0.8125\n",
      "Test Step: Batch 232, Loss: 0.4377742111682892, Acc: 0.84375\n",
      "Test Step: Batch 233, Loss: 0.16036631166934967, Acc: 0.9375\n",
      "Test Step: Batch 234, Loss: 0.3414066433906555, Acc: 0.9375\n",
      "Test Step: Batch 235, Loss: 0.17337581515312195, Acc: 0.90625\n",
      "Test Step: Batch 236, Loss: 0.2995243966579437, Acc: 0.90625\n",
      "Test Step: Batch 237, Loss: 0.26399466395378113, Acc: 0.90625\n",
      "Test Step: Batch 238, Loss: 0.46528780460357666, Acc: 0.8125\n",
      "Test Step: Batch 239, Loss: 0.3526511490345001, Acc: 0.875\n",
      "Test Step: Batch 240, Loss: 0.31710073351860046, Acc: 0.84375\n",
      "Test Step: Batch 241, Loss: 0.09166225790977478, Acc: 0.96875\n",
      "Test Step: Batch 242, Loss: 0.41657084226608276, Acc: 0.90625\n",
      "Test Step: Batch 243, Loss: 0.6498330235481262, Acc: 0.75\n",
      "Test Step: Batch 244, Loss: 0.5604321956634521, Acc: 0.8125\n",
      "Test Step: Batch 245, Loss: 0.3362411558628082, Acc: 0.875\n",
      "Test Step: Batch 246, Loss: 0.131795272231102, Acc: 0.96875\n",
      "Test Step: Batch 247, Loss: 0.26510316133499146, Acc: 0.90625\n",
      "Test Step: Batch 248, Loss: 0.4281843304634094, Acc: 0.875\n",
      "Test Step: Batch 249, Loss: 0.13839930295944214, Acc: 0.9375\n",
      "Test Step: Batch 250, Loss: 0.24935027956962585, Acc: 0.9375\n",
      "Test Step: Batch 251, Loss: 0.3254925012588501, Acc: 0.90625\n",
      "Test Step: Batch 252, Loss: 0.2887028455734253, Acc: 0.875\n",
      "Test Step: Batch 253, Loss: 0.3716791570186615, Acc: 0.84375\n",
      "Test Step: Batch 254, Loss: 0.2724286913871765, Acc: 0.875\n",
      "Test Step: Batch 255, Loss: 0.3010556101799011, Acc: 0.84375\n",
      "Test Step: Batch 256, Loss: 0.18537196516990662, Acc: 0.90625\n",
      "Test Step: Batch 257, Loss: 0.2624014914035797, Acc: 0.90625\n",
      "Test Step: Batch 258, Loss: 0.32074832916259766, Acc: 0.875\n",
      "Test Step: Batch 259, Loss: 0.3689015507698059, Acc: 0.78125\n",
      "Test Step: Batch 260, Loss: 0.39614880084991455, Acc: 0.875\n",
      "Test Step: Batch 261, Loss: 0.2167634665966034, Acc: 0.875\n",
      "Test Step: Batch 262, Loss: 0.37140214443206787, Acc: 0.875\n",
      "Test Step: Batch 263, Loss: 0.39660120010375977, Acc: 0.84375\n",
      "Test Step: Batch 264, Loss: 0.5795891880989075, Acc: 0.84375\n",
      "Test Step: Batch 265, Loss: 0.16866667568683624, Acc: 0.96875\n",
      "Test Step: Batch 266, Loss: 0.4065414071083069, Acc: 0.875\n",
      "Test Step: Batch 267, Loss: 0.285148561000824, Acc: 0.9375\n",
      "Test Step: Batch 268, Loss: 0.3216475546360016, Acc: 0.9375\n",
      "Test Step: Batch 269, Loss: 0.2655787765979767, Acc: 0.90625\n",
      "Test Step: Batch 270, Loss: 0.2883431613445282, Acc: 0.875\n",
      "Test Step: Batch 271, Loss: 0.26229792833328247, Acc: 0.875\n",
      "Test Step: Batch 272, Loss: 0.38961949944496155, Acc: 0.84375\n",
      "Test Step: Batch 273, Loss: 0.45761021971702576, Acc: 0.9375\n",
      "Test Step: Batch 274, Loss: 0.32775431871414185, Acc: 0.84375\n",
      "Test Step: Batch 275, Loss: 0.31811094284057617, Acc: 0.875\n",
      "Test Step: Batch 276, Loss: 0.2222936451435089, Acc: 0.90625\n",
      "Test Step: Batch 277, Loss: 0.23259691894054413, Acc: 0.90625\n",
      "Test Step: Batch 278, Loss: 0.18252012133598328, Acc: 0.90625\n",
      "Test Step: Batch 279, Loss: 0.3979572057723999, Acc: 0.84375\n",
      "Test Step: Batch 280, Loss: 0.17649713158607483, Acc: 0.90625\n",
      "Test Step: Batch 281, Loss: 0.24906228482723236, Acc: 0.90625\n",
      "Test Step: Batch 282, Loss: 0.34157609939575195, Acc: 0.90625\n",
      "Test Step: Batch 283, Loss: 0.2885150909423828, Acc: 0.90625\n",
      "Test Step: Batch 284, Loss: 0.19103920459747314, Acc: 0.9375\n",
      "Test Step: Batch 285, Loss: 0.2955749034881592, Acc: 0.875\n",
      "Test Step: Batch 286, Loss: 0.08685871958732605, Acc: 0.96875\n",
      "Test Step: Batch 287, Loss: 0.14775431156158447, Acc: 0.96875\n",
      "Test Step: Batch 288, Loss: 0.3638707101345062, Acc: 0.84375\n",
      "Test Step: Batch 289, Loss: 0.4319608807563782, Acc: 0.8125\n",
      "Test Step: Batch 290, Loss: 0.4832412600517273, Acc: 0.875\n",
      "Test Step: Batch 291, Loss: 0.0939471498131752, Acc: 0.96875\n",
      "Test Step: Batch 292, Loss: 0.36005234718322754, Acc: 0.8125\n",
      "Test Step: Batch 293, Loss: 0.3112395703792572, Acc: 0.90625\n",
      "Test Step: Batch 294, Loss: 0.3646007776260376, Acc: 0.8125\n",
      "Test Step: Batch 295, Loss: 0.1762387752532959, Acc: 0.9375\n",
      "Test Step: Batch 296, Loss: 0.2678655982017517, Acc: 0.90625\n",
      "Test Step: Batch 297, Loss: 0.5579708218574524, Acc: 0.8125\n",
      "Test Step: Batch 298, Loss: 0.3009745478630066, Acc: 0.875\n",
      "Test Step: Batch 299, Loss: 0.23727187514305115, Acc: 0.90625\n",
      "Test Step: Batch 300, Loss: 0.16200579702854156, Acc: 0.9375\n",
      "Test Step: Batch 301, Loss: 0.4537588655948639, Acc: 0.875\n",
      "Test Step: Batch 302, Loss: 0.20478829741477966, Acc: 0.90625\n",
      "Test Step: Batch 303, Loss: 0.29368746280670166, Acc: 0.90625\n",
      "Test Step: Batch 304, Loss: 0.3873634934425354, Acc: 0.8125\n",
      "Test Step: Batch 305, Loss: 0.35890233516693115, Acc: 0.9375\n",
      "Test Step: Batch 306, Loss: 0.4903513193130493, Acc: 0.8125\n",
      "Test Step: Batch 307, Loss: 0.3256390690803528, Acc: 0.90625\n",
      "Test Step: Batch 308, Loss: 0.31910955905914307, Acc: 0.9375\n",
      "Test Step: Batch 309, Loss: 0.22986677289009094, Acc: 0.90625\n",
      "Test Step: Batch 310, Loss: 0.36807507276535034, Acc: 0.90625\n",
      "Test Step: Batch 311, Loss: 0.40643808245658875, Acc: 0.84375\n",
      "Test Step: Batch 312, Loss: 0.17255786061286926, Acc: 0.9375\n",
      "Test Epoch End: Avg Loss: 0.32360298412676436, Avg Acc: 0.887979233226837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8878999948501587     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8879792094230652     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.32384464144706726    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.32360297441482544    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8878999948501587    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8879792094230652    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.32384464144706726   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.32360297441482544   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Accuracy: 0.8879, Pruned F1 Score: 0.8877\n",
      "[(0.8985, 0.9108), (0.8985, 0.8879)]\n",
      "[(0.8982989644479569, 0.9104842430179791), (0.8982989644479569, 0.8876966542139607)]\n",
      "Pruned model saved to: ./pruned_models/alexnet_pruned_30.pth\n",
      "Applying 50.0% pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 0.8985, Original F1 Score: 0.8983\n",
      "MODEL BEFORE PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Pruning with 50.0% percentage on 6 layers...\n",
      "Pruning layer: conv2\n",
      "Pruning layer: conv3\n",
      "Pruning layer: conv4\n",
      "Pruning layer: conv5\n",
      "Pruning layer: fc1\n",
      "Pruning layer: fc2\n",
      "MODEL AFTER PRUNING:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting fine-tuning of the pruned model...\n",
      "Epoch 1/5, Train Loss: 1388.8830, Train Acc: 0.6115, Val Loss: 220.1264, Val Acc: 0.7573\n",
      "fine_tune_train_loss: 1388.8830285072327, fine_tune_train_acc: 0.611525, fine_tune_val_loss: 220.1264233291149, fine_tune_val_acc: 0.7573\n",
      "Epoch 2/5, Train Loss: 893.2074, Train Acc: 0.7500, Val Loss: 185.0574, Val Acc: 0.7933\n",
      "fine_tune_train_loss: 893.2073603868484, fine_tune_train_acc: 0.750025, fine_tune_val_loss: 185.05742728710175, fine_tune_val_acc: 0.7933\n",
      "Epoch 3/5, Train Loss: 767.0126, Train Acc: 0.7880, Val Loss: 166.9243, Val Acc: 0.8132\n",
      "fine_tune_train_loss: 767.0125696361065, fine_tune_train_acc: 0.788025, fine_tune_val_loss: 166.924280539155, fine_tune_val_acc: 0.8132\n",
      "Epoch 4/5, Train Loss: 686.9507, Train Acc: 0.8087, Val Loss: 153.6208, Val Acc: 0.8288\n",
      "fine_tune_train_loss: 686.9507215768099, fine_tune_train_acc: 0.8087, fine_tune_val_loss: 153.62078788876534, fine_tune_val_acc: 0.8288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 633.0524, Train Acc: 0.8233, Val Loss: 148.7596, Val Acc: 0.8345\n",
      "fine_tune_train_loss: 633.0523573607206, fine_tune_train_acc: 0.8233, fine_tune_val_loss: 148.75962783396244, fine_tune_val_acc: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233f939d04544aacae367d46887be12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step: Batch 0, Loss: 0.20904754102230072, Acc: 0.96875\n",
      "Test Step: Batch 1, Loss: 0.6523450016975403, Acc: 0.75\n",
      "Test Step: Batch 2, Loss: 0.3476049304008484, Acc: 0.875\n",
      "Test Step: Batch 3, Loss: 0.2878001034259796, Acc: 0.96875\n",
      "Test Step: Batch 4, Loss: 0.6408233642578125, Acc: 0.78125\n",
      "Test Step: Batch 5, Loss: 0.5140109658241272, Acc: 0.84375\n",
      "Test Step: Batch 6, Loss: 0.37663474678993225, Acc: 0.875\n",
      "Test Step: Batch 7, Loss: 0.5945209264755249, Acc: 0.75\n",
      "Test Step: Batch 8, Loss: 0.5046756863594055, Acc: 0.78125\n",
      "Test Step: Batch 9, Loss: 0.39795100688934326, Acc: 0.90625\n",
      "Test Step: Batch 10, Loss: 0.28118735551834106, Acc: 0.875\n",
      "Test Step: Batch 11, Loss: 0.5293510556221008, Acc: 0.78125\n",
      "Test Step: Batch 12, Loss: 0.37157541513442993, Acc: 0.90625\n",
      "Test Step: Batch 13, Loss: 0.5949151515960693, Acc: 0.75\n",
      "Test Step: Batch 14, Loss: 0.5701407790184021, Acc: 0.78125\n",
      "Test Step: Batch 15, Loss: 0.24297665059566498, Acc: 0.9375\n",
      "Test Step: Batch 16, Loss: 0.29286670684814453, Acc: 0.84375\n",
      "Test Step: Batch 17, Loss: 0.295460969209671, Acc: 0.875\n",
      "Test Step: Batch 18, Loss: 0.22324958443641663, Acc: 0.9375\n",
      "Test Step: Batch 19, Loss: 0.3309362828731537, Acc: 0.9375\n",
      "Test Step: Batch 20, Loss: 0.6706205010414124, Acc: 0.8125\n",
      "Test Step: Batch 21, Loss: 0.41280946135520935, Acc: 0.875\n",
      "Test Step: Batch 22, Loss: 0.6220045685768127, Acc: 0.8125\n",
      "Test Step: Batch 23, Loss: 0.5997494459152222, Acc: 0.75\n",
      "Test Step: Batch 24, Loss: 0.5067389011383057, Acc: 0.84375\n",
      "Test Step: Batch 25, Loss: 0.5808181166648865, Acc: 0.78125\n",
      "Test Step: Batch 26, Loss: 0.676478385925293, Acc: 0.78125\n",
      "Test Step: Batch 27, Loss: 0.7124689221382141, Acc: 0.875\n",
      "Test Step: Batch 28, Loss: 0.48350977897644043, Acc: 0.84375\n",
      "Test Step: Batch 29, Loss: 0.4892802834510803, Acc: 0.84375\n",
      "Test Step: Batch 30, Loss: 0.33844098448753357, Acc: 0.9375\n",
      "Test Step: Batch 31, Loss: 0.5297250151634216, Acc: 0.75\n",
      "Test Step: Batch 32, Loss: 0.8544490933418274, Acc: 0.6875\n",
      "Test Step: Batch 33, Loss: 0.49300748109817505, Acc: 0.875\n",
      "Test Step: Batch 34, Loss: 0.3992759883403778, Acc: 0.8125\n",
      "Test Step: Batch 35, Loss: 0.5533522367477417, Acc: 0.8125\n",
      "Test Step: Batch 36, Loss: 0.39890968799591064, Acc: 0.875\n",
      "Test Step: Batch 37, Loss: 0.3609905540943146, Acc: 0.78125\n",
      "Test Step: Batch 38, Loss: 0.641033947467804, Acc: 0.71875\n",
      "Test Step: Batch 39, Loss: 0.46444422006607056, Acc: 0.78125\n",
      "Test Step: Batch 40, Loss: 0.40599334239959717, Acc: 0.84375\n",
      "Test Step: Batch 41, Loss: 0.32572758197784424, Acc: 0.90625\n",
      "Test Step: Batch 42, Loss: 0.40177419781684875, Acc: 0.8125\n",
      "Test Step: Batch 43, Loss: 0.7063296437263489, Acc: 0.78125\n",
      "Test Step: Batch 44, Loss: 0.21286824345588684, Acc: 0.9375\n",
      "Test Step: Batch 45, Loss: 0.4147818982601166, Acc: 0.84375\n",
      "Test Step: Batch 46, Loss: 0.8596842288970947, Acc: 0.6875\n",
      "Test Step: Batch 47, Loss: 0.5802661180496216, Acc: 0.8125\n",
      "Test Step: Batch 48, Loss: 0.500669002532959, Acc: 0.78125\n",
      "Test Step: Batch 49, Loss: 0.6504848599433899, Acc: 0.8125\n",
      "Test Step: Batch 50, Loss: 0.4368528127670288, Acc: 0.90625\n",
      "Test Step: Batch 51, Loss: 0.26830610632896423, Acc: 0.90625\n",
      "Test Step: Batch 52, Loss: 0.7512025237083435, Acc: 0.84375\n",
      "Test Step: Batch 53, Loss: 0.29766845703125, Acc: 0.90625\n",
      "Test Step: Batch 54, Loss: 0.6145070195198059, Acc: 0.75\n",
      "Test Step: Batch 55, Loss: 0.6811424493789673, Acc: 0.78125\n",
      "Test Step: Batch 56, Loss: 0.47698843479156494, Acc: 0.8125\n",
      "Test Step: Batch 57, Loss: 0.2659416198730469, Acc: 0.875\n",
      "Test Step: Batch 58, Loss: 0.6268460750579834, Acc: 0.78125\n",
      "Test Step: Batch 59, Loss: 0.3963446617126465, Acc: 0.84375\n",
      "Test Step: Batch 60, Loss: 0.8931098580360413, Acc: 0.71875\n",
      "Test Step: Batch 61, Loss: 0.6276435256004333, Acc: 0.78125\n",
      "Test Step: Batch 62, Loss: 0.5506976246833801, Acc: 0.71875\n",
      "Test Step: Batch 63, Loss: 0.6181567907333374, Acc: 0.78125\n",
      "Test Step: Batch 64, Loss: 0.531691312789917, Acc: 0.84375\n",
      "Test Step: Batch 65, Loss: 0.37313175201416016, Acc: 0.90625\n",
      "Test Step: Batch 66, Loss: 0.5387211441993713, Acc: 0.875\n",
      "Test Step: Batch 67, Loss: 0.8174369931221008, Acc: 0.75\n",
      "Test Step: Batch 68, Loss: 0.5336691737174988, Acc: 0.8125\n",
      "Test Step: Batch 69, Loss: 0.5773472785949707, Acc: 0.8125\n",
      "Test Step: Batch 70, Loss: 0.7679453492164612, Acc: 0.65625\n",
      "Test Step: Batch 71, Loss: 0.5089352130889893, Acc: 0.875\n",
      "Test Step: Batch 72, Loss: 0.6236240863800049, Acc: 0.8125\n",
      "Test Step: Batch 73, Loss: 0.6280608177185059, Acc: 0.8125\n",
      "Test Step: Batch 74, Loss: 0.4742961525917053, Acc: 0.8125\n",
      "Test Step: Batch 75, Loss: 0.4159846603870392, Acc: 0.90625\n",
      "Test Step: Batch 76, Loss: 0.493373841047287, Acc: 0.78125\n",
      "Test Step: Batch 77, Loss: 0.723518967628479, Acc: 0.78125\n",
      "Test Step: Batch 78, Loss: 0.8318798542022705, Acc: 0.78125\n",
      "Test Step: Batch 79, Loss: 0.7076495885848999, Acc: 0.8125\n",
      "Test Step: Batch 80, Loss: 0.6446577310562134, Acc: 0.75\n",
      "Test Step: Batch 81, Loss: 0.4249267876148224, Acc: 0.84375\n",
      "Test Step: Batch 82, Loss: 0.46582674980163574, Acc: 0.8125\n",
      "Test Step: Batch 83, Loss: 0.3569052219390869, Acc: 0.875\n",
      "Test Step: Batch 84, Loss: 0.5693747997283936, Acc: 0.75\n",
      "Test Step: Batch 85, Loss: 0.35319533944129944, Acc: 0.90625\n",
      "Test Step: Batch 86, Loss: 0.7033170461654663, Acc: 0.78125\n",
      "Test Step: Batch 87, Loss: 0.5160033702850342, Acc: 0.84375\n",
      "Test Step: Batch 88, Loss: 0.8770618438720703, Acc: 0.6875\n",
      "Test Step: Batch 89, Loss: 0.2450302243232727, Acc: 0.9375\n",
      "Test Step: Batch 90, Loss: 0.3293980658054352, Acc: 0.875\n",
      "Test Step: Batch 91, Loss: 0.2050178349018097, Acc: 0.96875\n",
      "Test Step: Batch 92, Loss: 0.47817888855934143, Acc: 0.8125\n",
      "Test Step: Batch 93, Loss: 0.6124536991119385, Acc: 0.78125\n",
      "Test Step: Batch 94, Loss: 0.5934900641441345, Acc: 0.8125\n",
      "Test Step: Batch 95, Loss: 0.5292008519172668, Acc: 0.84375\n",
      "Test Step: Batch 96, Loss: 0.5245322585105896, Acc: 0.84375\n",
      "Test Step: Batch 97, Loss: 0.4089992940425873, Acc: 0.84375\n",
      "Test Step: Batch 98, Loss: 0.4554775059223175, Acc: 0.875\n",
      "Test Step: Batch 99, Loss: 0.6086289882659912, Acc: 0.75\n",
      "Test Step: Batch 100, Loss: 0.665170431137085, Acc: 0.71875\n",
      "Test Step: Batch 101, Loss: 0.39402472972869873, Acc: 0.84375\n",
      "Test Step: Batch 102, Loss: 0.22949963808059692, Acc: 0.9375\n",
      "Test Step: Batch 103, Loss: 0.7641288042068481, Acc: 0.6875\n",
      "Test Step: Batch 104, Loss: 0.553979754447937, Acc: 0.75\n",
      "Test Step: Batch 105, Loss: 0.37503868341445923, Acc: 0.8125\n",
      "Test Step: Batch 106, Loss: 0.5563333034515381, Acc: 0.8125\n",
      "Test Step: Batch 107, Loss: 0.7401821613311768, Acc: 0.75\n",
      "Test Step: Batch 108, Loss: 0.32668471336364746, Acc: 0.84375\n",
      "Test Step: Batch 109, Loss: 0.530850887298584, Acc: 0.875\n",
      "Test Step: Batch 110, Loss: 0.40882760286331177, Acc: 0.90625\n",
      "Test Step: Batch 111, Loss: 0.5179516077041626, Acc: 0.8125\n",
      "Test Step: Batch 112, Loss: 0.666375994682312, Acc: 0.75\n",
      "Test Step: Batch 113, Loss: 0.3830857276916504, Acc: 0.84375\n",
      "Test Step: Batch 114, Loss: 0.4637775719165802, Acc: 0.875\n",
      "Test Step: Batch 115, Loss: 0.42366087436676025, Acc: 0.90625\n",
      "Test Step: Batch 116, Loss: 0.5318078994750977, Acc: 0.78125\n",
      "Test Step: Batch 117, Loss: 0.46778038144111633, Acc: 0.84375\n",
      "Test Step: Batch 118, Loss: 0.6193974018096924, Acc: 0.8125\n",
      "Test Step: Batch 119, Loss: 0.5656782388687134, Acc: 0.84375\n",
      "Test Step: Batch 120, Loss: 0.2915169596672058, Acc: 0.9375\n",
      "Test Step: Batch 121, Loss: 0.439170777797699, Acc: 0.78125\n",
      "Test Step: Batch 122, Loss: 0.38631492853164673, Acc: 0.90625\n",
      "Test Step: Batch 123, Loss: 0.35760778188705444, Acc: 0.875\n",
      "Test Step: Batch 124, Loss: 0.6412898302078247, Acc: 0.84375\n",
      "Test Step: Batch 125, Loss: 0.45784473419189453, Acc: 0.84375\n",
      "Test Step: Batch 126, Loss: 0.5831412076950073, Acc: 0.90625\n",
      "Test Step: Batch 127, Loss: 0.2779054343700409, Acc: 0.875\n",
      "Test Step: Batch 128, Loss: 0.6588696241378784, Acc: 0.78125\n",
      "Test Step: Batch 129, Loss: 0.6766570210456848, Acc: 0.84375\n",
      "Test Step: Batch 130, Loss: 0.2854699194431305, Acc: 0.875\n",
      "Test Step: Batch 131, Loss: 0.5686105489730835, Acc: 0.8125\n",
      "Test Step: Batch 132, Loss: 0.3830268383026123, Acc: 0.84375\n",
      "Test Step: Batch 133, Loss: 0.3208125829696655, Acc: 0.90625\n",
      "Test Step: Batch 134, Loss: 0.72447669506073, Acc: 0.78125\n",
      "Test Step: Batch 135, Loss: 0.2326197624206543, Acc: 0.90625\n",
      "Test Step: Batch 136, Loss: 0.3205149471759796, Acc: 0.90625\n",
      "Test Step: Batch 137, Loss: 0.5865324139595032, Acc: 0.875\n",
      "Test Step: Batch 138, Loss: 0.40501970052719116, Acc: 0.90625\n",
      "Test Step: Batch 139, Loss: 0.45980313420295715, Acc: 0.84375\n",
      "Test Step: Batch 140, Loss: 0.2978992462158203, Acc: 0.9375\n",
      "Test Step: Batch 141, Loss: 0.2237582951784134, Acc: 1.0\n",
      "Test Step: Batch 142, Loss: 0.658649206161499, Acc: 0.75\n",
      "Test Step: Batch 143, Loss: 0.3849840760231018, Acc: 0.875\n",
      "Test Step: Batch 144, Loss: 0.6365746259689331, Acc: 0.78125\n",
      "Test Step: Batch 145, Loss: 0.1883331835269928, Acc: 0.96875\n",
      "Test Step: Batch 146, Loss: 0.3227974772453308, Acc: 0.90625\n",
      "Test Step: Batch 147, Loss: 0.3377853333950043, Acc: 0.875\n",
      "Test Step: Batch 148, Loss: 0.9036325216293335, Acc: 0.71875\n",
      "Test Step: Batch 149, Loss: 0.6726809740066528, Acc: 0.6875\n",
      "Test Step: Batch 150, Loss: 0.3756459355354309, Acc: 0.8125\n",
      "Test Step: Batch 151, Loss: 0.42652347683906555, Acc: 0.90625\n",
      "Test Step: Batch 152, Loss: 0.43044114112854004, Acc: 0.90625\n",
      "Test Step: Batch 153, Loss: 0.607976496219635, Acc: 0.75\n",
      "Test Step: Batch 154, Loss: 0.5381327867507935, Acc: 0.78125\n",
      "Test Step: Batch 155, Loss: 0.8036963939666748, Acc: 0.71875\n",
      "Test Step: Batch 156, Loss: 0.510179877281189, Acc: 0.78125\n",
      "Test Step: Batch 157, Loss: 0.2902030348777771, Acc: 0.90625\n",
      "Test Step: Batch 158, Loss: 0.3121015727519989, Acc: 0.90625\n",
      "Test Step: Batch 159, Loss: 0.47508949041366577, Acc: 0.875\n",
      "Test Step: Batch 160, Loss: 0.3575330972671509, Acc: 0.90625\n",
      "Test Step: Batch 161, Loss: 0.4240562617778778, Acc: 0.78125\n",
      "Test Step: Batch 162, Loss: 0.7005773782730103, Acc: 0.78125\n",
      "Test Step: Batch 163, Loss: 0.45575278997421265, Acc: 0.8125\n",
      "Test Step: Batch 164, Loss: 0.39361539483070374, Acc: 0.8125\n",
      "Test Step: Batch 165, Loss: 0.30503901839256287, Acc: 0.9375\n",
      "Test Step: Batch 166, Loss: 0.37083032727241516, Acc: 0.9375\n",
      "Test Step: Batch 167, Loss: 0.4287108778953552, Acc: 0.8125\n",
      "Test Step: Batch 168, Loss: 0.3430575132369995, Acc: 0.875\n",
      "Test Step: Batch 169, Loss: 0.5591360330581665, Acc: 0.75\n",
      "Test Step: Batch 170, Loss: 0.5211960077285767, Acc: 0.8125\n",
      "Test Step: Batch 171, Loss: 0.41547906398773193, Acc: 0.90625\n",
      "Test Step: Batch 172, Loss: 0.7771339416503906, Acc: 0.8125\n",
      "Test Step: Batch 173, Loss: 0.7209492325782776, Acc: 0.78125\n",
      "Test Step: Batch 174, Loss: 0.6072589159011841, Acc: 0.8125\n",
      "Test Step: Batch 175, Loss: 0.6913473010063171, Acc: 0.78125\n",
      "Test Step: Batch 176, Loss: 0.8438361883163452, Acc: 0.6875\n",
      "Test Step: Batch 177, Loss: 0.23794905841350555, Acc: 0.9375\n",
      "Test Step: Batch 178, Loss: 0.3993348777294159, Acc: 0.84375\n",
      "Test Step: Batch 179, Loss: 0.5851926803588867, Acc: 0.75\n",
      "Test Step: Batch 180, Loss: 0.3773062825202942, Acc: 0.84375\n",
      "Test Step: Batch 181, Loss: 0.5364150404930115, Acc: 0.78125\n",
      "Test Step: Batch 182, Loss: 0.6915609836578369, Acc: 0.71875\n",
      "Test Step: Batch 183, Loss: 0.6412431001663208, Acc: 0.71875\n",
      "Test Step: Batch 184, Loss: 0.5957555174827576, Acc: 0.8125\n",
      "Test Step: Batch 185, Loss: 0.32766473293304443, Acc: 0.875\n",
      "Test Step: Batch 186, Loss: 0.33982792496681213, Acc: 0.90625\n",
      "Test Step: Batch 187, Loss: 0.557292640209198, Acc: 0.875\n",
      "Test Step: Batch 188, Loss: 0.44626584649086, Acc: 0.8125\n",
      "Test Step: Batch 189, Loss: 0.5646218061447144, Acc: 0.78125\n",
      "Test Step: Batch 190, Loss: 0.3299162685871124, Acc: 0.9375\n",
      "Test Step: Batch 191, Loss: 0.6156680583953857, Acc: 0.75\n",
      "Test Step: Batch 192, Loss: 0.4474439322948456, Acc: 0.8125\n",
      "Test Step: Batch 193, Loss: 0.4134417772293091, Acc: 0.875\n",
      "Test Step: Batch 194, Loss: 0.4444572329521179, Acc: 0.84375\n",
      "Test Step: Batch 195, Loss: 0.559637188911438, Acc: 0.78125\n",
      "Test Step: Batch 196, Loss: 0.21739089488983154, Acc: 0.96875\n",
      "Test Step: Batch 197, Loss: 0.3682100772857666, Acc: 0.90625\n",
      "Test Step: Batch 198, Loss: 0.29811975359916687, Acc: 0.9375\n",
      "Test Step: Batch 199, Loss: 0.45132845640182495, Acc: 0.90625\n",
      "Test Step: Batch 200, Loss: 0.6631186008453369, Acc: 0.78125\n",
      "Test Step: Batch 201, Loss: 0.419655442237854, Acc: 0.875\n",
      "Test Step: Batch 202, Loss: 0.3425939083099365, Acc: 0.875\n",
      "Test Step: Batch 203, Loss: 0.6226639747619629, Acc: 0.75\n",
      "Test Step: Batch 204, Loss: 0.5940903425216675, Acc: 0.78125\n",
      "Test Step: Batch 205, Loss: 0.6570154428482056, Acc: 0.78125\n",
      "Test Step: Batch 206, Loss: 0.1744844615459442, Acc: 0.9375\n",
      "Test Step: Batch 207, Loss: 0.36076247692108154, Acc: 0.8125\n",
      "Test Step: Batch 208, Loss: 0.4845409095287323, Acc: 0.90625\n",
      "Test Step: Batch 209, Loss: 0.37533003091812134, Acc: 0.84375\n",
      "Test Step: Batch 210, Loss: 0.3779360055923462, Acc: 0.84375\n",
      "Test Step: Batch 211, Loss: 0.6988226175308228, Acc: 0.78125\n",
      "Test Step: Batch 212, Loss: 0.36668723821640015, Acc: 0.875\n",
      "Test Step: Batch 213, Loss: 0.5036638975143433, Acc: 0.8125\n",
      "Test Step: Batch 214, Loss: 0.7487072944641113, Acc: 0.75\n",
      "Test Step: Batch 215, Loss: 0.3861790895462036, Acc: 0.84375\n",
      "Test Step: Batch 216, Loss: 0.46799659729003906, Acc: 0.78125\n",
      "Test Step: Batch 217, Loss: 0.6574125289916992, Acc: 0.75\n",
      "Test Step: Batch 218, Loss: 0.8753921389579773, Acc: 0.6875\n",
      "Test Step: Batch 219, Loss: 0.3705366253852844, Acc: 0.875\n",
      "Test Step: Batch 220, Loss: 0.6765499114990234, Acc: 0.8125\n",
      "Test Step: Batch 221, Loss: 0.308418333530426, Acc: 0.875\n",
      "Test Step: Batch 222, Loss: 0.48269423842430115, Acc: 0.78125\n",
      "Test Step: Batch 223, Loss: 0.28452593088150024, Acc: 0.9375\n",
      "Test Step: Batch 224, Loss: 0.5550992488861084, Acc: 0.78125\n",
      "Test Step: Batch 225, Loss: 0.5957933068275452, Acc: 0.84375\n",
      "Test Step: Batch 226, Loss: 0.25439393520355225, Acc: 0.9375\n",
      "Test Step: Batch 227, Loss: 0.47987765073776245, Acc: 0.875\n",
      "Test Step: Batch 228, Loss: 0.3897266983985901, Acc: 0.875\n",
      "Test Step: Batch 229, Loss: 0.3260424733161926, Acc: 0.875\n",
      "Test Step: Batch 230, Loss: 0.42896175384521484, Acc: 0.8125\n",
      "Test Step: Batch 231, Loss: 0.7690020799636841, Acc: 0.71875\n",
      "Test Step: Batch 232, Loss: 0.5509765148162842, Acc: 0.78125\n",
      "Test Step: Batch 233, Loss: 0.24403241276741028, Acc: 0.90625\n",
      "Test Step: Batch 234, Loss: 0.5262328386306763, Acc: 0.78125\n",
      "Test Step: Batch 235, Loss: 0.325602650642395, Acc: 0.90625\n",
      "Test Step: Batch 236, Loss: 0.5625574588775635, Acc: 0.78125\n",
      "Test Step: Batch 237, Loss: 0.4304819107055664, Acc: 0.8125\n",
      "Test Step: Batch 238, Loss: 0.6730198860168457, Acc: 0.8125\n",
      "Test Step: Batch 239, Loss: 0.4317716360092163, Acc: 0.90625\n",
      "Test Step: Batch 240, Loss: 0.4353794455528259, Acc: 0.875\n",
      "Test Step: Batch 241, Loss: 0.2557836174964905, Acc: 0.96875\n",
      "Test Step: Batch 242, Loss: 0.4712549149990082, Acc: 0.8125\n",
      "Test Step: Batch 243, Loss: 0.6181889772415161, Acc: 0.75\n",
      "Test Step: Batch 244, Loss: 0.6580138206481934, Acc: 0.71875\n",
      "Test Step: Batch 245, Loss: 0.46340304613113403, Acc: 0.8125\n",
      "Test Step: Batch 246, Loss: 0.35206684470176697, Acc: 0.90625\n",
      "Test Step: Batch 247, Loss: 0.5666390061378479, Acc: 0.78125\n",
      "Test Step: Batch 248, Loss: 0.607161283493042, Acc: 0.78125\n",
      "Test Step: Batch 249, Loss: 0.3284436762332916, Acc: 0.9375\n",
      "Test Step: Batch 250, Loss: 0.3686666488647461, Acc: 0.84375\n",
      "Test Step: Batch 251, Loss: 0.27853864431381226, Acc: 0.9375\n",
      "Test Step: Batch 252, Loss: 0.34658852219581604, Acc: 0.875\n",
      "Test Step: Batch 253, Loss: 0.5707048773765564, Acc: 0.8125\n",
      "Test Step: Batch 254, Loss: 0.5416383743286133, Acc: 0.78125\n",
      "Test Step: Batch 255, Loss: 0.3302648067474365, Acc: 0.875\n",
      "Test Step: Batch 256, Loss: 0.6092391610145569, Acc: 0.75\n",
      "Test Step: Batch 257, Loss: 0.4659082889556885, Acc: 0.84375\n",
      "Test Step: Batch 258, Loss: 0.45866698026657104, Acc: 0.71875\n",
      "Test Step: Batch 259, Loss: 0.7027326822280884, Acc: 0.71875\n",
      "Test Step: Batch 260, Loss: 0.5456247329711914, Acc: 0.75\n",
      "Test Step: Batch 261, Loss: 0.3745516538619995, Acc: 0.875\n",
      "Test Step: Batch 262, Loss: 0.5381908416748047, Acc: 0.75\n",
      "Test Step: Batch 263, Loss: 0.5649865865707397, Acc: 0.8125\n",
      "Test Step: Batch 264, Loss: 0.723017692565918, Acc: 0.78125\n",
      "Test Step: Batch 265, Loss: 0.3953685462474823, Acc: 0.84375\n",
      "Test Step: Batch 266, Loss: 0.5582191944122314, Acc: 0.78125\n",
      "Test Step: Batch 267, Loss: 0.46796470880508423, Acc: 0.8125\n",
      "Test Step: Batch 268, Loss: 0.5119919776916504, Acc: 0.84375\n",
      "Test Step: Batch 269, Loss: 0.6317777037620544, Acc: 0.75\n",
      "Test Step: Batch 270, Loss: 0.519317626953125, Acc: 0.84375\n",
      "Test Step: Batch 271, Loss: 0.3544524013996124, Acc: 0.84375\n",
      "Test Step: Batch 272, Loss: 0.5586010217666626, Acc: 0.8125\n",
      "Test Step: Batch 273, Loss: 0.5197961330413818, Acc: 0.8125\n",
      "Test Step: Batch 274, Loss: 0.40658819675445557, Acc: 0.90625\n",
      "Test Step: Batch 275, Loss: 0.39052078127861023, Acc: 0.875\n",
      "Test Step: Batch 276, Loss: 0.3548789620399475, Acc: 0.90625\n",
      "Test Step: Batch 277, Loss: 0.3152018189430237, Acc: 0.875\n",
      "Test Step: Batch 278, Loss: 0.5104997158050537, Acc: 0.875\n",
      "Test Step: Batch 279, Loss: 0.644899845123291, Acc: 0.71875\n",
      "Test Step: Batch 280, Loss: 0.3868378698825836, Acc: 0.875\n",
      "Test Step: Batch 281, Loss: 0.3221309185028076, Acc: 0.90625\n",
      "Test Step: Batch 282, Loss: 0.4587157964706421, Acc: 0.8125\n",
      "Test Step: Batch 283, Loss: 0.5650538206100464, Acc: 0.84375\n",
      "Test Step: Batch 284, Loss: 0.2520596385002136, Acc: 0.96875\n",
      "Test Step: Batch 285, Loss: 0.31388258934020996, Acc: 0.875\n",
      "Test Step: Batch 286, Loss: 0.23987716436386108, Acc: 0.9375\n",
      "Test Step: Batch 287, Loss: 0.3139238655567169, Acc: 0.84375\n",
      "Test Step: Batch 288, Loss: 0.8114391565322876, Acc: 0.6875\n",
      "Test Step: Batch 289, Loss: 0.6924469470977783, Acc: 0.71875\n",
      "Test Step: Batch 290, Loss: 0.728520393371582, Acc: 0.78125\n",
      "Test Step: Batch 291, Loss: 0.2666979432106018, Acc: 0.90625\n",
      "Test Step: Batch 292, Loss: 0.6733540892601013, Acc: 0.78125\n",
      "Test Step: Batch 293, Loss: 0.46388858556747437, Acc: 0.8125\n",
      "Test Step: Batch 294, Loss: 0.4847966432571411, Acc: 0.8125\n",
      "Test Step: Batch 295, Loss: 0.593528151512146, Acc: 0.8125\n",
      "Test Step: Batch 296, Loss: 0.4201264977455139, Acc: 0.8125\n",
      "Test Step: Batch 297, Loss: 0.5049015879631042, Acc: 0.875\n",
      "Test Step: Batch 298, Loss: 0.3981696367263794, Acc: 0.875\n",
      "Test Step: Batch 299, Loss: 0.3674026131629944, Acc: 0.90625\n",
      "Test Step: Batch 300, Loss: 0.29277467727661133, Acc: 0.90625\n",
      "Test Step: Batch 301, Loss: 0.4796200096607208, Acc: 0.875\n",
      "Test Step: Batch 302, Loss: 0.2763882279396057, Acc: 0.90625\n",
      "Test Step: Batch 303, Loss: 0.46235623955726624, Acc: 0.84375\n",
      "Test Step: Batch 304, Loss: 0.6489730477333069, Acc: 0.71875\n",
      "Test Step: Batch 305, Loss: 0.5153253674507141, Acc: 0.8125\n",
      "Test Step: Batch 306, Loss: 0.58982914686203, Acc: 0.75\n",
      "Test Step: Batch 307, Loss: 0.3681100904941559, Acc: 0.90625\n",
      "Test Step: Batch 308, Loss: 0.6732242107391357, Acc: 0.78125\n",
      "Test Step: Batch 309, Loss: 0.48703092336654663, Acc: 0.78125\n",
      "Test Step: Batch 310, Loss: 0.7309191226959229, Acc: 0.75\n",
      "Test Step: Batch 311, Loss: 0.6590454578399658, Acc: 0.75\n",
      "Test Step: Batch 312, Loss: 0.5537753701210022, Acc: 0.8125\n",
      "Test Epoch End: Avg Loss: 0.4914166183707813, Avg Acc: 0.830870607028754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8309000134468079     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8308706283569336     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.49131646752357483    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4914166331291199     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8309000134468079    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8308706283569336    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.49131646752357483   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4914166331291199    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Accuracy: 0.8309, Pruned F1 Score: 0.8297\n",
      "[(0.8985, 0.9108), (0.8985, 0.8879), (0.8985, 0.8309)]\n",
      "[(0.8982989644479569, 0.9104842430179791), (0.8982989644479569, 0.8876966542139607), (0.8982989644479569, 0.8296746897035231)]\n",
      "Pruned model saved to: ./pruned_models/alexnet_pruned_50.pth\n",
      "Metrics Debug:\n",
      "Pruning Percentages: [50.0]\n",
      "Test Accuracy: [0.830870607028754]\n",
      "Test Loss: [0.4914166183707813]\n",
      "Model Size: [14368042]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁</td></tr><tr><td>test_acc_batch</td><td>█▆▁</td></tr><tr><td>test_acc_epoch</td><td>█▆▁</td></tr><tr><td>test_loss_batch</td><td>▁▃█</td></tr><tr><td>test_loss_epoch</td><td>▁▃█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>test_acc_batch</td><td>0.8309</td></tr><tr><td>test_acc_epoch</td><td>0.83087</td></tr><tr><td>test_loss_batch</td><td>0.49132</td></tr><tr><td>test_loss_epoch</td><td>0.49142</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_Run</strong> at: <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/i1sbpiwk' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/i1sbpiwk</a><br/> View project at: <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241208_200938-i1sbpiwk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    wandb.init(project='alexnet_depGraph', name='AlexNet_Run')\n",
    "    wandb_logger = WandbLogger(log_model=False)\n",
    "\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    full_train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Create train/validation split\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    checkpoint_path = \"../checkpointsAlex/checkpoint_with_aug_epoch5.ckpt\"\n",
    "\n",
    "    # Define pruning percentages\n",
    "    # pruning_percentages = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    pruning_percentages = [0.1, 0.3, 0.5]\n",
    "\n",
    "    os.makedirs('./pruned_models/', exist_ok=True)\n",
    "\n",
    "    for pruning_percentage in pruning_percentages:\n",
    "        print(f\"Applying {pruning_percentage * 100}% pruning...\")\n",
    "        \n",
    "        # Reload the original model from checkpoint each time\n",
    "        model = AlexNetFineTuner.load_from_checkpoint(checkpoint_path).to(\"mps\").to(torch.float32)\n",
    "\n",
    "        # Evaluate before pruning\n",
    "        orig_accuracy, orig_f1 = evaluate_model(model, test_dataloader)\n",
    "        print(f\"Original Accuracy: {orig_accuracy:.4f}, Original F1 Score: {orig_f1:.4f}\")\n",
    "\n",
    "        # Prune the model and fine-tune it (fine-tuning is done inside prune_model if train_loader and val_loader are passed)\n",
    "        model.prune_model(pruning_percentage=pruning_percentage, \n",
    "                          train_loader=train_dataloader, \n",
    "                          val_loader=val_dataloader, \n",
    "                          fine_tune_epochs=5, \n",
    "                          fine_tune_lr=1e-5)\n",
    "        model.to(\"mps\")\n",
    "\n",
    "        # Test the pruned (and potentially fine-tuned) model\n",
    "        trainer = pl.Trainer(max_epochs=5, logger=wandb_logger, accelerator='mps')\n",
    "        trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "        model.to(\"mps\").to(torch.float32)\n",
    "        pruned_accuracy, pruned_f1 = evaluate_model(model, test_dataloader)\n",
    "        print(f\"Pruned Accuracy: {pruned_accuracy:.4f}, Pruned F1 Score: {pruned_f1:.4f}\")\n",
    "\n",
    "        # Store accuracies and F1 scores for plotting\n",
    "        accuracies.append((orig_accuracy, pruned_accuracy))\n",
    "        f1_scores.append((orig_f1, pruned_f1))\n",
    "\n",
    "        print(accuracies)\n",
    "        print(f1_scores)\n",
    "        \n",
    "        # Save the pruned model to disk\n",
    "        model.zero_grad()  # Clear gradients to reduce file size\n",
    "        pruned_model_path = f\"./pruned_models/alexnet_pruned_{int(pruning_percentage * 100)}.pth\"\n",
    "        torch.save(model, pruned_model_path)\n",
    "        print(f\"Pruned model saved to: {pruned_model_path}\")\n",
    "\n",
    "    # Plot metrics after all pruning iterations\n",
    "    plot_metrics(model.metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pruned Models: ['./pruned_models/alexnet_pruned_10.pth', './pruned_models/alexnet_pruned_0.pth', './pruned_models/alexnet_pruned_20.pth', './pruned_models/alexnet_pruned_30.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def list_pruned_models(directory='./pruned_models/'):\n",
    "    # List all files in the directory\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.pth')]\n",
    "\n",
    "pruned_models = list_pruned_models()\n",
    "print(\"Available Pruned Models:\", pruned_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pruned model from: ./pruned_models/alexnet_pruned_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/wpc2_w2s73z_8qr0dbq576x00000gn/T/ipykernel_81511/2172303324.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetFineTuner(\n",
      "  (model): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 173, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(173, 346, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(346, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(231, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=8316, out_features=3687, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=3687, out_features=3687, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=3687, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL AFTER REBUILDING:\n",
      " AlexNetFineTuner(\n",
      "  (model): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 173, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(173, 346, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(346, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(231, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=8316, out_features=3687, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=3687, out_features=3687, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=3687, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def load_pruned_model(model_path):\n",
    "    print(f\"Loading pruned model from: {model_path}\")\n",
    "    model = torch.load(model_path)\n",
    "    model = model.to(\"mps\").to(torch.float32)  # Move to MPS if needed\n",
    "    return model\n",
    "\n",
    "# Example: Load a pruned model with 30% pruning\n",
    "pruned_model_path = \"./pruned_models/alexnet_pruned_10.pth\"\n",
    "pruned_model = load_pruned_model(pruned_model_path)\n",
    "\n",
    "# Verify the loaded model\n",
    "print(pruned_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
