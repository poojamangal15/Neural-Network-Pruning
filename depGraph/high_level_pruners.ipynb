{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torch_pruning as tp\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Set MPS as the default device\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetFineTuner(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4, num_classes=10):\n",
    "        super(AlexNetFineTuner, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load pre-trained AlexNet\n",
    "        self.model = models.alexnet(pretrained=False)\n",
    "        self.model.classifier[6] = torch.nn.Linear(4096, num_classes)  # Update the classifier layer\n",
    "\n",
    "        # Initialize metrics dictionary\n",
    "        self.metrics = {\n",
    "            \"pruning_percentage\": [],\n",
    "            \"test_accuracy\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"model_size\": []\n",
    "        }\n",
    "        \n",
    "        self.test_outputs = []\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def print_dependency_graph(self, DG):\n",
    "        print(\"\\nDependency Graph Details:\")\n",
    "        for module, node in DG.module2node.items():\n",
    "            print(f\"Module: {module}\")\n",
    "            for dep in node.dependencies:\n",
    "                print(f\"    * Target Module: {dep.target.module}\")\n",
    "\n",
    "    def visualize_dependency_graph(self, DG):\n",
    "        \"\"\"Visualize the dependency graph using networkx.\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        for module, node in DG.module2node.items():\n",
    "            for dep in node.dependencies:\n",
    "                G.add_edge(str(module), str(dep.target.module))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, with_labels=True, node_size=1000, font_size=8, node_color=\"skyblue\", edge_color=\"gray\")\n",
    "        plt.title(\"Dependency Graph\")\n",
    "        plt.show()\n",
    "\n",
    "    # def prune_model(self, pruning_percentage=0.2):\n",
    "    #     # example_inputs = torch.randn(1, 3, 224, 224)  # Example input for dependency graph\n",
    "    #     example_inputs = torch.randn(1, 3, 224, 224, dtype=torch.float32).to(\"mps\")\n",
    "    #     imp = tp.importance.TaylorImportance()\n",
    "\n",
    "    #     ignored_layers = []\n",
    "    #     for m in model.modules():\n",
    "    #         if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "    #             ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "    #     iterative_steps = 5 # progressive pruning\n",
    "    #     pruner = tp.pruner.MagnitudePruner(\n",
    "    #         model,\n",
    "    #         example_inputs,\n",
    "    #         importance=imp,\n",
    "    #         iterative_steps=iterative_steps,\n",
    "    #         ch_sparsity=0.5, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    #         ignored_layers=ignored_layers,\n",
    "    #     )\n",
    "        \n",
    "    #     self.model = self.model.to(\"mps\")\n",
    "    #     self.model = self.model.to(torch.float32)\n",
    "\n",
    "    #     print(\"MODEL BEFORE PRUNING --------------------->\", self.model)\n",
    "\n",
    "    #     DG = tp.DependencyGraph().build_dependency(self.model, example_inputs)\n",
    "    #     self.print_dependency_graph(DG)\n",
    "    #     self.visualize_dependency_graph(DG)\n",
    "\n",
    "    #     # Choose a layer to prune\n",
    "    #     layer_to_prune = self.model.classifier[1]\n",
    "\n",
    "    #     # Prune layer\n",
    "    #     num_features = layer_to_prune.out_features\n",
    "    #     pruning_idxs = torch.arange(0, int(num_features * pruning_percentage)).tolist()\n",
    "    #     group = DG.get_pruning_group(layer_to_prune, tp.prune_linear_out_channels, idxs=pruning_idxs)\n",
    "    #     if DG.check_pruning_group(group):\n",
    "    #         group.prune()\n",
    "\n",
    "    #     print(\"MODEL AFTER PRUNING --------------------->\", self.model)\n",
    "    #     print(\"PRUNING GROUP ---------------------->\", group)\n",
    "\n",
    "    #     all_groups = list(DG.get_all_groups())\n",
    "    #     print(\"Number of Groups: --------------------->%d\"%len(all_groups))\n",
    "    #     print(\"The last Group: --------------------->\", all_groups[-1])\n",
    "    #     # Update metrics after pruning\n",
    "    #     self.metrics[\"model_size\"].append(sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
    "    #     self.metrics[\"pruning_percentage\"].append(pruning_percentage * 100)\n",
    "\n",
    "    def high_level_prune_model(self, ch_sparsity=0.5, iterative_steps=5, example_inputs=None):\n",
    "        if example_inputs is None:\n",
    "            example_inputs = torch.randn(1, 3, 224, 224, dtype=torch.float32).to(\"mps\")\n",
    "        \n",
    "        print(f\"Initial model state -> Device: {next(self.model.parameters()).device}, Dtype: {next(self.model.parameters()).dtype}\")\n",
    "        self.model = self.model.to(\"mps\").to(torch.float32)\n",
    "\n",
    "        # Define importance criteria\n",
    "        imp = tp.importance.TaylorImportance()\n",
    "        print(\"IMPORTANCE CRITERIA------------------->\", imp)\n",
    "\n",
    "        # Specify ignored layers (e.g., the final classification layer)\n",
    "        ignored_layers = []\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, torch.nn.Linear) and m.out_features == 10:  # Adjust for CIFAR-10\n",
    "                ignored_layers.append(m)\n",
    "\n",
    "        # Initialize high-level pruner\n",
    "        pruner = tp.pruner.MagnitudePruner(\n",
    "            self.model,\n",
    "            example_inputs,\n",
    "            importance=imp,\n",
    "            iterative_steps=iterative_steps,\n",
    "            ch_sparsity=ch_sparsity,\n",
    "            ignored_layers=ignored_layers,\n",
    "        )\n",
    "\n",
    "        # Count initial MACs and parameters\n",
    "        base_macs, base_nparams = tp.utils.count_ops_and_params(self.model, example_inputs)\n",
    "        print(f\"Initial MACs: {base_macs}, Parameters: {base_nparams}\")\n",
    "\n",
    "        # Iteratively prune and update metrics\n",
    "        for i in range(iterative_steps):\n",
    "            print(f\"Pruning step {i + 1}/{iterative_steps}...\")\n",
    "\n",
    "            # If using TaylorImportance, calculate gradients\n",
    "            if isinstance(imp, tp.importance.TaylorImportance):\n",
    "                self.model.zero_grad()  # Clear gradients\n",
    "                loss = self.model(example_inputs).sum()  # Dummy loss\n",
    "                loss.backward()  # Backpropagate to calculate gradients\n",
    "\n",
    "            # Prune the model\n",
    "            pruner.step()\n",
    "\n",
    "            # Recalculate MACs and parameters\n",
    "            macs, nparams = tp.utils.count_ops_and_params(self.model, example_inputs)\n",
    "            print(f\"After step {i + 1}: MACs={macs}, Params={nparams}\")\n",
    "\n",
    "            # Update metrics\n",
    "            self.metrics[\"model_size\"].append(nparams)\n",
    "            self.metrics[\"pruning_percentage\"].append((ch_sparsity * 100 * (i + 1) / iterative_steps))\n",
    "\n",
    "        print(\"Pruning complete.\")\n",
    "        # Final model stats\n",
    "        final_macs, final_nparams = tp.utils.count_ops_and_params(self.model, example_inputs)\n",
    "        print(f\"Final MACs: {final_macs}, Parameters: {final_nparams}\")\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)  # Convert inputs to float32\n",
    "        labels = labels.to(\"mps\")  # Ensure labels are on MPS\n",
    "\n",
    "        print(\"Images device and dtype:---------------------\", images.device, images.dtype)\n",
    "        print(\"Labels device and dtype:\", labels.device, labels.dtype)\n",
    "\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)  # Convert inputs to float32\n",
    "        labels = labels.to(\"mps\")  # Ensure labels are on MPS\n",
    "        outputs = self(images)\n",
    "        val_loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_acc = (preds == labels).float().mean()\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)\n",
    "        labels = labels.to(\"mps\")\n",
    "        outputs = self(images)\n",
    "        test_loss = F.cross_entropy(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_acc = (preds == labels).float().mean()\n",
    "\n",
    "        # Store batch results for aggregation later\n",
    "        self.test_outputs.append({\"test_loss\": test_loss.item(), \"test_acc\": test_acc.item()})\n",
    "\n",
    "        # Log per-batch results if needed\n",
    "        self.log(\"test_loss_batch\", test_loss, prog_bar=True)\n",
    "        self.log(\"test_acc_batch\", test_acc, prog_bar=True)\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Aggregate metrics across batches\n",
    "        avg_loss = sum(o[\"test_loss\"] for o in self.test_outputs) / len(self.test_outputs)\n",
    "        avg_acc = sum(o[\"test_acc\"] for o in self.test_outputs) / len(self.test_outputs)\n",
    "\n",
    "        # Append aggregated metrics for the epoch\n",
    "        self.metrics[\"test_accuracy\"].append(avg_acc)\n",
    "        self.metrics[\"test_loss\"].append(avg_loss)\n",
    "\n",
    "        # Log aggregated metrics\n",
    "        self.log(\"test_loss_epoch\", avg_loss, prog_bar=True)\n",
    "        self.log(\"test_acc_epoch\", avg_acc, prog_bar=True)\n",
    "\n",
    "        # Clear outputs for the next test epoch\n",
    "        self.test_outputs = []\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    metrics[\"pruning_percentage\"] = [0, 0.1,0.2,0.3,0.4,0.5]\n",
    "    print(\"----------------------->1\", metrics[\"pruning_percentage\"], metrics[\"test_accuracy[1,7]\"])\n",
    "    print(\"----------------------->2\", metrics[\"pruning_percentage\"], metrics[\"test_loss\"])\n",
    "    print(\"----------------------->2\", metrics[\"pruning_percentage\"], metrics[\"model_size\"])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"test_accuracy\"], marker='o', label=\"Accuracy\")\n",
    "    plt.title(\"Test Accuracy vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"test_loss\"], marker='o', color=\"orange\", label=\"Loss\")\n",
    "    plt.title(\"Test Loss vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"pruning_percentage\"], metrics[\"model_size\"], marker='o', color=\"green\", label=\"Model Size\")\n",
    "    plt.title(\"Model Size vs. Pruning Percentage\")\n",
    "    plt.xlabel(\"Pruning Percentage (%)\")\n",
    "    plt.ylabel(\"Number of Parameters\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mp-mangal\u001b[0m (\u001b[33mp-mangal-university-of-amsterdam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/poojamangal/Desktop/Masters/NNProject/depGraph/wandb/run-20241202_110008-ezq7hx89</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/ezq7hx89' target=\"_blank\">AlexNet_HighLevelPruning</a></strong> to <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/ezq7hx89' target=\"_blank\">https://wandb.ai/p-mangal-university-of-amsterdam/alexnet_depGraph/runs/ezq7hx89</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model (0% pruning)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc1101b34ce4dfc9e754cdbf49833c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8700000047683716     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.8984375         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.34062659740448      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.26861053705215454    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8700000047683716    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.8984375        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.34062659740448     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.26861053705215454   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30eaa54f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/metapruner.py:90: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\"ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial MACs: 711505866.0, Parameters: 57044810\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=711505866.0, Params=57044810\n",
      "Pruning complete.\n",
      "Final MACs: 711505866.0, Parameters: 57044810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84d28d1385a481ea4782ffbd51d7bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8700000047683716     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.8984375         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.34062659740448      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.26861053705215454    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8700000047683716    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.8984375        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.34062659740448     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.26861053705215454   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 10.0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30e7a76e0>\n",
      "Initial MACs: 711505866.0, Parameters: 57044810\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=578047534.0, Params=46142053\n",
      "Pruning complete.\n",
      "Final MACs: 578047534.0, Parameters: 46142053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93582535e2b44262943feee359edfd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7400000095367432     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.796875          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7792699933052063     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6969653367996216     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7400000095367432    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.796875         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7792699933052063    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6969653367996216    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 20.0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30e9feb40>\n",
      "Initial MACs: 578047534.0, Parameters: 46142053\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=377242916.0, Params=29526996\n",
      "Pruning complete.\n",
      "Final MACs: 377242916.0, Parameters: 29526996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3458b01fe514e278235d0c26c1353e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4399999976158142     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.3984375         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.8462207317352295     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.962432861328125     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4399999976158142    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.3984375        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.8462207317352295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.962432861328125    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 30.0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30e644b90>\n",
      "Initial MACs: 377242916.0, Parameters: 29526996\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=192553590.0, Params=14407299\n",
      "Pruning complete.\n",
      "Final MACs: 192553590.0, Parameters: 14407299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de22cbcabf54cf5bc1f43d9c3941e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14000000059604645    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.1640625         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.356874704360962     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.3838493824005127     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14000000059604645   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.1640625        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.356874704360962    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.3838493824005127    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 40.0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30d34aea0>\n",
      "Initial MACs: 192553590.0, Parameters: 14407299\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=75838955.0, Params=5151620\n",
      "Pruning complete.\n",
      "Final MACs: 75838955.0, Parameters: 5151620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69544455afaf40faaa7175679c03f143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10000000149011612    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.078125          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.7451839447021484     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.7895078659057617     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10000000149011612   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.078125         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7451839447021484    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7895078659057617    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying pruning at 50.0%...\n",
      "Initial model state -> Device: cpu, Dtype: torch.float32\n",
      "IMPORTANCE CRITERIA-------------------> <torch_pruning.pruner.importance.TaylorImportance object at 0x30d34aea0>\n",
      "Initial MACs: 75838955.0, Parameters: 5151620\n",
      "Pruning step 1/1...\n",
      "After step 1: MACs=23825366.0, Params=1291365\n",
      "Pruning complete.\n",
      "Final MACs: 23825366.0, Parameters: 1291365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7b439d18ce408aa48c3edbc273011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_batch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10000000149011612    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.078125          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_batch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.466773748397827     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.5068798065185547     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_batch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10000000149011612   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.078125         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_batch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.466773748397827    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.5068798065185547    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'test_accuracy[1,7]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtest(model, dataloaders\u001b[38;5;241m=\u001b[39mtest_dataloader)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Plot Metrics\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mplot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mplot_metrics\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_metrics\u001b[39m(metrics):\n\u001b[1;32m      2\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruning_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.4\u001b[39m,\u001b[38;5;241m0.5\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------->1\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruning_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_accuracy[1,7]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------->2\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruning_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m], metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------->2\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruning_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m], metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_accuracy[1,7]'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    wandb.init(project='alexnet_depGraph', name='AlexNet_HighLevelPruning')\n",
    "    wandb_logger = WandbLogger(log_model=False)\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    test_dataset = Subset(CIFAR10(root='./data', train=False, download=True, transform=transform), range(100))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    checkpoint_path = \"../checkpointsAlex/best-checkpoint.ckpt\"\n",
    "    model = AlexNetFineTuner.load_from_checkpoint(checkpoint_path)\n",
    "    model = model.to(torch.float32).to(\"mps\")\n",
    "\n",
    "    # Baseline test (0% pruning)\n",
    "    print(\"Evaluating baseline model (0% pruning)...\")\n",
    "    trainer = pl.Trainer(logger=wandb_logger, max_epochs=1)\n",
    "    trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "    # Pruning\n",
    "    pruning_percentages = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for pruning_percentage in pruning_percentages:\n",
    "        print(f\"Applying pruning at {pruning_percentage * 100}%...\")\n",
    "        model.high_level_prune_model(ch_sparsity=pruning_percentage, iterative_steps=1)\n",
    "        trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(model.metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
