{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/poojamangal/Desktop/Masters/NNProject/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.functional import accuracy\n",
    "from models.alexNet_fineTuner import AlexNetFineTuner\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from torchmetrics.functional import accuracy\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 (AlexNet input size)\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(           # Normalize with ImageNet means and stds\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "full_train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training dataset into train/validation\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define pruning percentages\n",
    "# pruning_percentages = [0.1, 0.3, 0.5]\n",
    "pruning_percentages = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model and returns the accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Append predictions and labels to lists\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    return accuracy, f1\n",
    "\n",
    "\n",
    "# Fine-tune model function\n",
    "def fine_tune_model(model, train_dataloader, val_dataloader, epochs=5, learning_rate=1e-5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, val_correct = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(train_dataloader.dataset)\n",
    "        val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Visualize weights function\n",
    "def visualize_weights(weights, title, save_path=\"./plots\", name=\"weights.png\"):\n",
    "    out_channels, in_channels, height, width = weights.shape\n",
    "    n_cols = min(8, out_channels)  # Set the number of columns\n",
    "    n_rows = (out_channels + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for i in range(out_channels):\n",
    "        kernel = weights[i, 0].cpu().detach().numpy()\n",
    "        ax = axes[i // n_cols, i % n_cols] if n_rows > 1 else axes[i % n_cols]\n",
    "        ax.imshow(kernel, cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i}\")\n",
    "    for j in range(i + 1, n_rows * n_cols):\n",
    "        fig.delaxes(axes.flatten()[j])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}/{name}\")\n",
    "    print(f\"Pruned weights visualization saved: {save_path}/{name}\")\n",
    "\n",
    "# Get non-zero indices function\n",
    "def get_nonzero_indices(model):\n",
    "    non_zero_indices = {}\n",
    "    new_channels = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            weights = module.weight.data\n",
    "            indices = [i for i in range(weights.size(0)) if weights[i].abs().sum() > 0]\n",
    "            normalized_name = name.replace('model.', '')\n",
    "            non_zero_indices[normalized_name] = indices\n",
    "            out_channels = len(indices)\n",
    "            in_channels = weights.size(1) if len(weights.shape) > 2 else weights.shape[0]\n",
    "            new_channels[normalized_name] = (out_channels, in_channels)\n",
    "    return non_zero_indices, new_channels\n",
    "\n",
    "# Create dense model function\n",
    "def create_dense_model(new_channels):\n",
    "    class DenseAlexNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(DenseAlexNet, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, new_channels[\"features.0\"][0], kernel_size=11, stride=4, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(new_channels[\"features.0\"][0], new_channels[\"features.3\"][0], kernel_size=5, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(new_channels[\"features.3\"][0], new_channels[\"features.6\"][0], kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(new_channels[\"features.6\"][0], new_channels[\"features.8\"][0], kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(new_channels[\"features.8\"][0], new_channels[\"features.10\"][0], kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(new_channels[\"features.10\"][0] * 6 * 6, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 10),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "    return DenseAlexNet()\n",
    "\n",
    "# Function to copy weights from pruned to dense model\n",
    "def copy_weights(pruned_model, dense_model, non_zero_indices):\n",
    "    for (name, module), (_, dense_module) in zip(pruned_model.named_modules(), dense_model.named_modules()):\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            if name in non_zero_indices:\n",
    "                indices = non_zero_indices[name]\n",
    "                for idx_out, idx_in in indices:\n",
    "                    dense_module.weight.data[idx_out].copy_(module.weight.data[idx_out])\n",
    "                if module.bias is not None:\n",
    "                    dense_module.bias.data.copy_(module.bias.data)\n",
    "\n",
    "# Plotting results\n",
    "def plot_comparison(val_accuracy, val_f1_score, fine_tune_accuracy, fine_tune_f1_score, exp_name, prune_ratio):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(['Accuracy Before', 'F1 Score Before', 'Accuracy After', 'F1 Score After'],\n",
    "            [val_accuracy, val_f1_score, fine_tune_accuracy, fine_tune_f1_score],\n",
    "            color=['skyblue', 'salmon', 'skyblue', 'salmon'])\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"Evaluation Metrics Before and After Fine-Tuning (Prune Ratio: {prune_ratio*100:.0f}%)\")\n",
    "    os.makedirs(\"./plots\", exist_ok=True)\n",
    "    plt.savefig(f\"./plots/{exp_name}_Accuracy_Comparison_{str(int(prune_ratio*100))}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def get_pruned_indices_and_counts(model):\n",
    "    \"\"\"\n",
    "    Analyze a structurally pruned model to determine:\n",
    "    - The indices of pruned channels (dim 0 and dim 1).\n",
    "    - The number of pruned channels in each layer.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "    - pruned_info: A dictionary containing pruned indices and counts for each layer.\n",
    "    \"\"\"\n",
    "    pruned_info = {}\n",
    "    pruned_weights = {}\n",
    "    num_pruned_channels = {}\n",
    "    \n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, nn.Conv2d):  # Check for Conv2d layers\n",
    "            layer_info = {}\n",
    "            \n",
    "            # Get the weight tensor\n",
    "            weights = layer.weight.detach()\n",
    "            print(\"LAYER----------\", layer)\n",
    "            print(\"LAYER.weight----------\", layer.weight)\n",
    "            print(\"LAYER.weight.detach----------\", layer.weight.detach)\n",
    "            \n",
    "            # Identify pruned filters along dim=0 (output channels)\n",
    "            pruned_dim0 = torch.nonzero(weights.abs().sum(dim=(1, 2, 3)) == 0).squeeze().tolist()\n",
    "            if isinstance(pruned_dim0, int):  # Handle single index case\n",
    "                pruned_dim0 = [pruned_dim0]\n",
    "            layer_info['pruned_dim0'] = pruned_dim0\n",
    "            \n",
    "            \n",
    "            # Identify pruned filters along dim=1 (input channels)\n",
    "            pruned_dim1 = torch.nonzero(weights.abs().sum(dim=(0, 2, 3)) == 0).squeeze().tolist()\n",
    "            if isinstance(pruned_dim1, int):  # Handle single index case\n",
    "                pruned_dim1 = [pruned_dim1]\n",
    "            layer_info['pruned_dim1'] = pruned_dim1\n",
    "            \n",
    "            # Add layer information to pruned_info\n",
    "            pruned_info[name] = layer_info\n",
    "            num_pruned_channels[name] = (len(pruned_dim0), len(pruned_dim1))\n",
    "            \n",
    "            pruned_weights[name] = weights[pruned_dim0][:,pruned_dim1,:,:]\n",
    "    \n",
    "    return pruned_info, num_pruned_channels, pruned_weights\n",
    "\n",
    "def get_unpruned_indices_and_counts(model):\n",
    "    non_pruned_info = {}\n",
    "    num_unpruned_channels = {}\n",
    "    unpruned_weights = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            \n",
    "            layer_info = {}\n",
    "            \n",
    "            # Get the weight tensor\n",
    "            weights = module.weight.detach()\n",
    "            \n",
    "            # Identify pruned filters along dim=0 (output channels)\n",
    "            pruned_dim0 = torch.nonzero(weights.abs().sum(dim=(1, 2, 3)) > 0).squeeze().tolist()\n",
    "            if isinstance(pruned_dim0, int):  # Handle single index case\n",
    "                pruned_dim0 = [pruned_dim0]\n",
    "            layer_info['pruned_dim0'] = pruned_dim0\n",
    "            \n",
    "            \n",
    "            # Identify pruned filters along dim=1 (input channels)\n",
    "            pruned_dim1 = torch.nonzero(weights.abs().sum(dim=(0, 2, 3)) > 0).squeeze().tolist()\n",
    "            if isinstance(pruned_dim1, int):  # Handle single index case\n",
    "                pruned_dim1 = [pruned_dim1]\n",
    "            layer_info['pruned_dim1'] = pruned_dim1\n",
    "        \n",
    "            non_pruned_info[name] = layer_info\n",
    "            num_unpruned_channels[name] = (len(pruned_dim0), len(pruned_dim1))\n",
    "            unpruned_weights[name] = weights[pruned_dim0][:,pruned_dim1,:,:]\n",
    "                            \n",
    "                        \n",
    "    return non_pruned_info, num_unpruned_channels, unpruned_weights\n",
    "\n",
    "    \n",
    "def count_zero_weights(model):\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_weights += param.numel()\n",
    "            zero_weights += (param == 0).sum().item()\n",
    "    print(f\"Total weights: {total_weights}\")\n",
    "    print(f\"Zero weights: {zero_weights}\")\n",
    "    print(f\"Percentage of pruned weights: {100 * zero_weights / total_weights:.2f}%\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poojamangal/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/poojamangal/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weights: 57035456\n",
      "Zero weights: 28517728\n",
      "Percentage of pruned weights: 50.00%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_pruned_indices_and_counts() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m count_zero_weights(model)\n\u001b[1;32m     21\u001b[0m nonzero , new_channels \u001b[38;5;241m=\u001b[39m get_nonzero_indices(model)\n\u001b[0;32m---> 23\u001b[0m pruned_info, num_pruned_channels, pruned_weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_pruned_indices_and_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_pruned_info, num_unpruned_channels, unpruned_weights \u001b[38;5;241m=\u001b[39m get_unpruned_indices_and_counts(model)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(f\"Fine-tuning the model after pruning {amount * 100:.0f}% of weights...\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# fine_tune_model(model, train_dataloader, val_dataloader, epochs=3, learning_rate=1e-5)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     prune_ratio=amount\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_pruned_indices_and_counts() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for amount in pruning_percentages:\n",
    "    checkpoint_path = \"checkpoints/best_checkpoint.ckpt\"\n",
    "    model = AlexNetFineTuner.load_from_checkpoint(checkpoint_path).to(device)\n",
    "\n",
    "     # Evaluate the model before pruning\n",
    "    # orig_accuracy, orig_f1_score = evaluate_model(model, test_dataloader)\n",
    "    # print(f\"Original Accuracy before pruning: {orig_accuracy:.4f}\")\n",
    "    # print(f\"Original F1 Score before pruning: {orig_f1_score:.4f}\")\n",
    "\n",
    "    if amount > 0:\n",
    "        parameters_to_prune = [(module, 'weight') for name, module in model.named_modules() if isinstance(module, (nn.Conv2d, nn.Linear))]\n",
    "        prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=amount)\n",
    "    \n",
    "    for module, param_name in parameters_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "\n",
    "    count_zero_weights(model)\n",
    "    nonzero , new_channels = get_nonzero_indices(model)\n",
    "    \n",
    "    pruned_info, num_pruned_channels, pruned_weights = get_pruned_indices_and_counts(model)\n",
    "    non_pruned_info, num_unpruned_channels, unpruned_weights = get_unpruned_indices_and_counts(model)\n",
    "\n",
    "    # print(f\"Fine-tuning the model after pruning {amount * 100:.0f}% of weights...\")\n",
    "    # fine_tune_model(model, train_dataloader, val_dataloader, epochs=3, learning_rate=1e-5)\n",
    "\n",
    "    # Evaluate the fine-tuned model\n",
    "    # fine_tuned_accuracy, fine_tuned_f1_score = evaluate_model(model, test_dataloader)\n",
    "    # print(f\"Fine-tuned Accuracy after pruning {amount * 100:.0f}%: {fine_tuned_accuracy:.4f}\")\n",
    "    # print(f\"Fine-tuned F1 Score after pruning {amount * 100:.0f}%: {fine_tuned_f1_score:.4f}\")\n",
    "    \n",
    "    # Store accuracies and F1 scores for plotting\n",
    "    # accuracies.append((orig_accuracy, fine_tuned_accuracy))\n",
    "    # f1_scores.append((orig_f1_score, fine_tuned_f1_score))\n",
    "    \n",
    "    # non_zero_indices, new_channels = get_nonzero_indices(model)\n",
    "    \n",
    "    # for name, module in model.named_modules():\n",
    "    #     if isinstance(module, nn.Conv2d):\n",
    "    #         visualize_weights(module.weight, f\"Pruned Weights in {name}\", name=f\"{name.replace('.', '_')}_pruned_weights.png\")\n",
    "    \n",
    "    # dense_model = create_dense_model(new_channels)\n",
    "    # copy_weights(model, dense_model, non_zero_indices)\n",
    "\n",
    "   # Verify the dense model\n",
    "    # print(\"Original Model:\", model)\n",
    "    # print(\"Dense Model:\", dense_model)\n",
    "    # print(f\"Pruning {amount * 100:.0f}% Complete!\")\n",
    "\n",
    "    # Plot comparison of accuracy and F1 score before and after fine-tuning\n",
    "    # plot_comparison(\n",
    "    #     val_accuracy=orig_accuracy,\n",
    "    #     val_f1_score=orig_f1_score,\n",
    "    #     fine_tune_accuracy=fine_tuned_accuracy,\n",
    "    #     fine_tune_f1_score=fine_tuned_f1_score,\n",
    "    #     exp_name=f\"Pruning_{amount * 100:.0f}\",\n",
    "    #     prune_ratio=amount\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.features.0': tensor([], size=(0, 0, 11, 11)), 'model.features.3': tensor([], size=(0, 0, 5, 5)), 'model.features.6': tensor([], size=(0, 0, 3, 3)), 'model.features.8': tensor([], size=(0, 0, 3, 3)), 'model.features.10': tensor([], size=(0, 0, 3, 3))}\n"
     ]
    }
   ],
   "source": [
    "print(pruned_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.features.0': (0, 0), 'model.features.3': (0, 0), 'model.features.6': (0, 0), 'model.features.8': (0, 0), 'model.features.10': (0, 0)}\n"
     ]
    }
   ],
   "source": [
    "print(num_pruned_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.features.0': {'pruned_dim0': [], 'pruned_dim1': []}, 'model.features.3': {'pruned_dim0': [], 'pruned_dim1': []}, 'model.features.6': {'pruned_dim0': [], 'pruned_dim1': []}, 'model.features.8': {'pruned_dim0': [], 'pruned_dim1': []}, 'model.features.10': {'pruned_dim0': [], 'pruned_dim1': []}}\n"
     ]
    }
   ],
   "source": [
    "print(pruned_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.features.0': {'pruned_dim0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], 'pruned_dim1': [0, 1, 2]}, 'model.features.3': {'pruned_dim0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191], 'pruned_dim1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]}, 'model.features.6': {'pruned_dim0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383], 'pruned_dim1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]}, 'model.features.8': {'pruned_dim0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], 'pruned_dim1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]}, 'model.features.10': {'pruned_dim0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], 'pruned_dim1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]}}\n"
     ]
    }
   ],
   "source": [
    "print(non_pruned_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.features.0': (64, 3),\n",
       " 'model.features.3': (192, 64),\n",
       " 'model.features.6': (384, 192),\n",
       " 'model.features.8': (256, 384),\n",
       " 'model.features.10': (256, 256)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unpruned_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.features.0': tensor([[[[ 0.0401,  0.0000,  0.0401,  ..., -0.0290,  0.0119, -0.0238],\n",
       "           [ 0.0195, -0.0310, -0.0540,  ..., -0.0208, -0.0000,  0.0247],\n",
       "           [-0.0455,  0.0484,  0.0283,  ...,  0.0000, -0.0515, -0.0482],\n",
       "           ...,\n",
       "           [-0.0258,  0.0305,  0.0299,  ...,  0.0000, -0.0327, -0.0457],\n",
       "           [ 0.0233, -0.0344, -0.0374,  ..., -0.0411,  0.0206, -0.0000],\n",
       "           [-0.0120, -0.0535,  0.0259,  ..., -0.0479, -0.0435, -0.0219]],\n",
       " \n",
       "          [[-0.0098, -0.0191, -0.0277,  ...,  0.0357,  0.0114,  0.0448],\n",
       "           [-0.0123, -0.0401,  0.0416,  ...,  0.0286, -0.0185, -0.0408],\n",
       "           [-0.0068, -0.0236,  0.0289,  ..., -0.0082, -0.0420,  0.0137],\n",
       "           ...,\n",
       "           [ 0.0369,  0.0195, -0.0195,  ...,  0.0403,  0.0275, -0.0496],\n",
       "           [ 0.0266, -0.0514,  0.0137,  ..., -0.0294,  0.0382,  0.0308],\n",
       "           [-0.0393,  0.0282,  0.0136,  ..., -0.0216, -0.0437,  0.0314]],\n",
       " \n",
       "          [[-0.0334, -0.0000,  0.0518,  ...,  0.0374,  0.0082,  0.0230],\n",
       "           [-0.0401,  0.0530, -0.0278,  ..., -0.0000, -0.0066,  0.0000],\n",
       "           [-0.0176, -0.0457,  0.0490,  ..., -0.0352, -0.0307, -0.0386],\n",
       "           ...,\n",
       "           [ 0.0499, -0.0490,  0.0476,  ...,  0.0259,  0.0337, -0.0082],\n",
       "           [ 0.0351, -0.0191,  0.0206,  ...,  0.0159,  0.0511, -0.0448],\n",
       "           [-0.0418, -0.0320,  0.0000,  ...,  0.0288,  0.0234, -0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0289,  0.0319, -0.0328,  ...,  0.0239, -0.0303,  0.0377],\n",
       "           [ 0.0135, -0.0000,  0.0404,  ..., -0.0375, -0.0444,  0.0349],\n",
       "           [ 0.0290, -0.0146,  0.0472,  ...,  0.0177, -0.0284, -0.0464],\n",
       "           ...,\n",
       "           [-0.0377, -0.0000, -0.0422,  ...,  0.0514, -0.0221, -0.0155],\n",
       "           [ 0.0175, -0.0122,  0.0196,  ..., -0.0186,  0.0100,  0.0092],\n",
       "           [-0.0449,  0.0354,  0.0000,  ...,  0.0305,  0.0136,  0.0163]],\n",
       " \n",
       "          [[-0.0441, -0.0444,  0.0157,  ..., -0.0447, -0.0202, -0.0492],\n",
       "           [-0.0499,  0.0106,  0.0000,  ...,  0.0062, -0.0208,  0.0226],\n",
       "           [-0.0354,  0.0386, -0.0138,  ..., -0.0432,  0.0177,  0.0520],\n",
       "           ...,\n",
       "           [ 0.0233,  0.0397, -0.0526,  ...,  0.0063, -0.0137, -0.0236],\n",
       "           [ 0.0311,  0.0074,  0.0340,  ...,  0.0208, -0.0531,  0.0383],\n",
       "           [-0.0189, -0.0349, -0.0386,  ...,  0.0110, -0.0129, -0.0509]],\n",
       " \n",
       "          [[ 0.0356, -0.0201, -0.0130,  ...,  0.0431,  0.0502, -0.0250],\n",
       "           [ 0.0000,  0.0068,  0.0215,  ...,  0.0517, -0.0490, -0.0487],\n",
       "           [-0.0391,  0.0345, -0.0228,  ...,  0.0190,  0.0070,  0.0066],\n",
       "           ...,\n",
       "           [ 0.0150,  0.0494,  0.0500,  ..., -0.0000, -0.0271, -0.0000],\n",
       "           [ 0.0335, -0.0534, -0.0000,  ...,  0.0146, -0.0000, -0.0000],\n",
       "           [ 0.0321,  0.0172, -0.0423,  ..., -0.0186, -0.0247,  0.0061]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0209,  0.0106, -0.0087,  ...,  0.0325,  0.0289,  0.0184],\n",
       "           [ 0.0529,  0.0067,  0.0122,  ...,  0.0512, -0.0357,  0.0203],\n",
       "           [-0.0321, -0.0000,  0.0256,  ..., -0.0380, -0.0372,  0.0505],\n",
       "           ...,\n",
       "           [-0.0000, -0.0102, -0.0132,  ..., -0.0480, -0.0442,  0.0403],\n",
       "           [ 0.0293,  0.0237, -0.0343,  ..., -0.0215, -0.0165, -0.0349],\n",
       "           [-0.0429,  0.0491, -0.0257,  ...,  0.0207,  0.0085, -0.0515]],\n",
       " \n",
       "          [[ 0.0235, -0.0410,  0.0106,  ...,  0.0485,  0.0286, -0.0000],\n",
       "           [-0.0212,  0.0459, -0.0314,  ...,  0.0190, -0.0418, -0.0165],\n",
       "           [ 0.0317, -0.0393,  0.0000,  ..., -0.0085, -0.0094, -0.0276],\n",
       "           ...,\n",
       "           [-0.0000,  0.0233,  0.0346,  ...,  0.0000, -0.0515,  0.0194],\n",
       "           [ 0.0290,  0.0090,  0.0122,  ..., -0.0175, -0.0206,  0.0206],\n",
       "           [-0.0356, -0.0475,  0.0510,  ...,  0.0356,  0.0062,  0.0126]],\n",
       " \n",
       "          [[-0.0443, -0.0365,  0.0504,  ...,  0.0328, -0.0000,  0.0452],\n",
       "           [-0.0435,  0.0408, -0.0068,  ..., -0.0357, -0.0455,  0.0502],\n",
       "           [ 0.0490,  0.0197, -0.0407,  ...,  0.0473,  0.0507, -0.0245],\n",
       "           ...,\n",
       "           [-0.0362,  0.0065, -0.0168,  ...,  0.0251,  0.0410, -0.0101],\n",
       "           [-0.0126,  0.0498, -0.0085,  ..., -0.0474,  0.0170,  0.0185],\n",
       "           [-0.0430, -0.0000, -0.0481,  ..., -0.0413, -0.0123,  0.0385]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0448, -0.0084, -0.0510,  ...,  0.0000, -0.0149, -0.0138],\n",
       "           [-0.0128,  0.0497,  0.0508,  ..., -0.0210,  0.0131,  0.0095],\n",
       "           [-0.0084,  0.0291,  0.0169,  ..., -0.0154, -0.0157,  0.0375],\n",
       "           ...,\n",
       "           [ 0.0149, -0.0520,  0.0449,  ..., -0.0472, -0.0194,  0.0344],\n",
       "           [ 0.0501,  0.0429, -0.0500,  ..., -0.0000, -0.0507, -0.0093],\n",
       "           [ 0.0415, -0.0181, -0.0091,  ..., -0.0402,  0.0386,  0.0076]],\n",
       " \n",
       "          [[-0.0107,  0.0340, -0.0063,  ..., -0.0447,  0.0442, -0.0422],\n",
       "           [ 0.0490,  0.0454, -0.0415,  ...,  0.0286,  0.0415,  0.0102],\n",
       "           [ 0.0207, -0.0000,  0.0464,  ..., -0.0000, -0.0000, -0.0370],\n",
       "           ...,\n",
       "           [ 0.0375, -0.0496,  0.0000,  ..., -0.0184, -0.0000,  0.0075],\n",
       "           [-0.0106, -0.0250,  0.0249,  ..., -0.0348, -0.0408,  0.0474],\n",
       "           [-0.0305,  0.0312,  0.0000,  ..., -0.0453,  0.0443,  0.0303]],\n",
       " \n",
       "          [[ 0.0414,  0.0467, -0.0143,  ...,  0.0000,  0.0000,  0.0356],\n",
       "           [-0.0325,  0.0118, -0.0299,  ...,  0.0224, -0.0063,  0.0431],\n",
       "           [-0.0501,  0.0249, -0.0132,  ...,  0.0424, -0.0000, -0.0298],\n",
       "           ...,\n",
       "           [ 0.0373, -0.0365,  0.0000,  ..., -0.0461,  0.0517, -0.0387],\n",
       "           [ 0.0374,  0.0416, -0.0486,  ..., -0.0302,  0.0459, -0.0444],\n",
       "           [ 0.0543,  0.0477, -0.0374,  ...,  0.0468,  0.0000,  0.0418]]],\n",
       " \n",
       " \n",
       "         [[[-0.0111, -0.0000,  0.0270,  ...,  0.0163,  0.0170, -0.0249],\n",
       "           [ 0.0250,  0.0239,  0.0474,  ...,  0.0150,  0.0136,  0.0113],\n",
       "           [ 0.0081, -0.0078,  0.0292,  ...,  0.0211,  0.0289, -0.0124],\n",
       "           ...,\n",
       "           [-0.0396, -0.0159,  0.0293,  ..., -0.0188, -0.0443, -0.0426],\n",
       "           [ 0.0358,  0.0199, -0.0315,  ..., -0.0000,  0.0397,  0.0339],\n",
       "           [ 0.0456,  0.0481,  0.0000,  ..., -0.0095,  0.0065,  0.0222]],\n",
       " \n",
       "          [[ 0.0000,  0.0185, -0.0424,  ...,  0.0000, -0.0361,  0.0082],\n",
       "           [-0.0406,  0.0233, -0.0525,  ...,  0.0127,  0.0381, -0.0223],\n",
       "           [-0.0270,  0.0000,  0.0115,  ..., -0.0000, -0.0188,  0.0333],\n",
       "           ...,\n",
       "           [-0.0352, -0.0000,  0.0319,  ...,  0.0064,  0.0473, -0.0449],\n",
       "           [ 0.0378,  0.0000,  0.0383,  ...,  0.0201, -0.0194, -0.0419],\n",
       "           [-0.0504,  0.0330,  0.0420,  ..., -0.0290,  0.0240,  0.0158]],\n",
       " \n",
       "          [[-0.0061, -0.0146,  0.0211,  ..., -0.0082, -0.0117, -0.0522],\n",
       "           [-0.0394, -0.0412, -0.0371,  ..., -0.0393,  0.0093, -0.0134],\n",
       "           [ 0.0191,  0.0302, -0.0427,  ..., -0.0133, -0.0101, -0.0376],\n",
       "           ...,\n",
       "           [-0.0487, -0.0000, -0.0460,  ..., -0.0000, -0.0079,  0.0380],\n",
       "           [ 0.0226, -0.0177, -0.0089,  ..., -0.0479, -0.0257,  0.0085],\n",
       "           [ 0.0482,  0.0281, -0.0476,  ...,  0.0068, -0.0091,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0514, -0.0000,  0.0285,  ...,  0.0520,  0.0000, -0.0496],\n",
       "           [ 0.0213, -0.0465, -0.0389,  ..., -0.0199,  0.0405,  0.0369],\n",
       "           [ 0.0133, -0.0448, -0.0270,  ..., -0.0157, -0.0099,  0.0000],\n",
       "           ...,\n",
       "           [-0.0067, -0.0289,  0.0116,  ..., -0.0351,  0.0443,  0.0153],\n",
       "           [-0.0131,  0.0192, -0.0139,  ...,  0.0228, -0.0000, -0.0319],\n",
       "           [ 0.0105,  0.0386, -0.0242,  ...,  0.0105, -0.0487, -0.0150]],\n",
       " \n",
       "          [[-0.0217,  0.0082, -0.0481,  ...,  0.0064, -0.0458,  0.0403],\n",
       "           [ 0.0000, -0.0000, -0.0529,  ..., -0.0229,  0.0252,  0.0195],\n",
       "           [-0.0426, -0.0282, -0.0291,  ...,  0.0000, -0.0193, -0.0327],\n",
       "           ...,\n",
       "           [ 0.0252, -0.0094,  0.0000,  ...,  0.0198, -0.0424, -0.0487],\n",
       "           [ 0.0090, -0.0076, -0.0000,  ...,  0.0481, -0.0236,  0.0144],\n",
       "           [-0.0099, -0.0000, -0.0384,  ...,  0.0249, -0.0356, -0.0252]],\n",
       " \n",
       "          [[-0.0481,  0.0188,  0.0000,  ...,  0.0135, -0.0274, -0.0115],\n",
       "           [-0.0154, -0.0409,  0.0421,  ..., -0.0000,  0.0296,  0.0374],\n",
       "           [-0.0125, -0.0484, -0.0383,  ...,  0.0119, -0.0483, -0.0254],\n",
       "           ...,\n",
       "           [ 0.0138,  0.0282, -0.0429,  ...,  0.0151,  0.0362,  0.0491],\n",
       "           [ 0.0319, -0.0259, -0.0181,  ...,  0.0116, -0.0454, -0.0358],\n",
       "           [ 0.0160, -0.0239,  0.0099,  ..., -0.0470,  0.0131,  0.0425]]]]),\n",
       " 'model.features.3': tensor([[[[ 0.0134, -0.0211,  0.0136, -0.0104, -0.0100],\n",
       "           [ 0.0000, -0.0189,  0.0088,  0.0230,  0.0086],\n",
       "           [-0.0143,  0.0000, -0.0000,  0.0199, -0.0232],\n",
       "           [ 0.0227, -0.0207, -0.0155,  0.0149,  0.0126],\n",
       "           [-0.0000,  0.0075,  0.0204,  0.0122, -0.0158]],\n",
       " \n",
       "          [[-0.0070,  0.0197,  0.0132,  0.0000, -0.0113],\n",
       "           [ 0.0096,  0.0187,  0.0079, -0.0076,  0.0000],\n",
       "           [-0.0000,  0.0174,  0.0201, -0.0077, -0.0203],\n",
       "           [ 0.0000,  0.0000,  0.0187, -0.0178,  0.0231],\n",
       "           [-0.0000,  0.0000, -0.0000,  0.0068,  0.0066]],\n",
       " \n",
       "          [[ 0.0113, -0.0168, -0.0113, -0.0000,  0.0073],\n",
       "           [ 0.0000, -0.0146, -0.0107, -0.0206,  0.0226],\n",
       "           [-0.0000,  0.0171,  0.0214, -0.0000,  0.0198],\n",
       "           [ 0.0068,  0.0000,  0.0110, -0.0097,  0.0165],\n",
       "           [ 0.0099, -0.0213, -0.0101,  0.0115, -0.0076]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0217,  0.0255, -0.0000,  0.0000, -0.0000],\n",
       "           [-0.0137,  0.0130, -0.0116, -0.0216,  0.0000],\n",
       "           [-0.0167, -0.0000, -0.0215,  0.0193,  0.0000],\n",
       "           [-0.0209, -0.0000,  0.0222,  0.0000, -0.0208],\n",
       "           [-0.0176, -0.0166,  0.0000, -0.0186, -0.0224]],\n",
       " \n",
       "          [[ 0.0103,  0.0197, -0.0187, -0.0000,  0.0238],\n",
       "           [-0.0084,  0.0000, -0.0082,  0.0000,  0.0105],\n",
       "           [ 0.0129, -0.0215, -0.0154, -0.0067,  0.0155],\n",
       "           [-0.0110, -0.0083, -0.0131,  0.0169,  0.0214],\n",
       "           [-0.0000, -0.0185, -0.0153, -0.0194,  0.0097]],\n",
       " \n",
       "          [[ 0.0164, -0.0073,  0.0180,  0.0113,  0.0140],\n",
       "           [-0.0069, -0.0162,  0.0146,  0.0127,  0.0088],\n",
       "           [ 0.0101,  0.0000, -0.0157,  0.0102, -0.0200],\n",
       "           [ 0.0087, -0.0227,  0.0116, -0.0176, -0.0067],\n",
       "           [-0.0114, -0.0138, -0.0104, -0.0244, -0.0208]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000, -0.0000,  0.0087, -0.0162, -0.0217],\n",
       "           [ 0.0214, -0.0000, -0.0000,  0.0143, -0.0237],\n",
       "           [ 0.0062, -0.0198, -0.0214, -0.0195,  0.0121],\n",
       "           [ 0.0000, -0.0208, -0.0117, -0.0152, -0.0102],\n",
       "           [-0.0000,  0.0115,  0.0067,  0.0127,  0.0191]],\n",
       " \n",
       "          [[-0.0136, -0.0084, -0.0195,  0.0000, -0.0000],\n",
       "           [ 0.0155, -0.0151, -0.0213,  0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0098, -0.0000,  0.0123,  0.0093],\n",
       "           [ 0.0233,  0.0186,  0.0174,  0.0179,  0.0000],\n",
       "           [-0.0196,  0.0144,  0.0000, -0.0000, -0.0179]],\n",
       " \n",
       "          [[ 0.0235,  0.0084,  0.0000,  0.0227, -0.0099],\n",
       "           [-0.0078,  0.0000, -0.0127, -0.0126,  0.0000],\n",
       "           [ 0.0196,  0.0236, -0.0108,  0.0191,  0.0150],\n",
       "           [ 0.0094, -0.0164,  0.0000,  0.0098,  0.0000],\n",
       "           [-0.0136,  0.0179,  0.0217,  0.0242, -0.0083]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0088, -0.0121, -0.0199,  0.0226,  0.0150],\n",
       "           [ 0.0174, -0.0153, -0.0064, -0.0102, -0.0142],\n",
       "           [ 0.0172,  0.0238, -0.0201, -0.0271, -0.0250],\n",
       "           [ 0.0000, -0.0084, -0.0000, -0.0000,  0.0112],\n",
       "           [ 0.0000, -0.0208, -0.0000,  0.0105, -0.0230]],\n",
       " \n",
       "          [[ 0.0153,  0.0063,  0.0084,  0.0000, -0.0219],\n",
       "           [ 0.0135, -0.0251, -0.0274,  0.0179, -0.0186],\n",
       "           [ 0.0194, -0.0163, -0.0275, -0.0062,  0.0000],\n",
       "           [ 0.0109, -0.0270, -0.0262,  0.0104,  0.0155],\n",
       "           [-0.0237, -0.0229, -0.0000,  0.0152, -0.0000]],\n",
       " \n",
       "          [[-0.0089,  0.0000,  0.0103, -0.0000,  0.0000],\n",
       "           [-0.0000,  0.0143, -0.0000,  0.0109, -0.0184],\n",
       "           [-0.0172,  0.0000, -0.0140, -0.0000, -0.0141],\n",
       "           [ 0.0229, -0.0247,  0.0087,  0.0077, -0.0000],\n",
       "           [ 0.0088, -0.0170, -0.0000,  0.0188,  0.0233]]],\n",
       " \n",
       " \n",
       "         [[[-0.0185,  0.0181, -0.0143, -0.0194,  0.0098],\n",
       "           [-0.0000,  0.0143, -0.0177,  0.0000, -0.0164],\n",
       "           [-0.0134,  0.0107, -0.0000,  0.0138, -0.0073],\n",
       "           [ 0.0063,  0.0185,  0.0095,  0.0239, -0.0223],\n",
       "           [-0.0000,  0.0059, -0.0211,  0.0220, -0.0128]],\n",
       " \n",
       "          [[ 0.0128,  0.0183,  0.0205,  0.0095,  0.0000],\n",
       "           [ 0.0075,  0.0061,  0.0066, -0.0000, -0.0221],\n",
       "           [-0.0109,  0.0000,  0.0222,  0.0000, -0.0161],\n",
       "           [ 0.0235,  0.0000, -0.0000, -0.0181, -0.0108],\n",
       "           [ 0.0103,  0.0199, -0.0114, -0.0154,  0.0247]],\n",
       " \n",
       "          [[-0.0000, -0.0168,  0.0000,  0.0209, -0.0179],\n",
       "           [ 0.0168,  0.0137,  0.0135, -0.0136,  0.0194],\n",
       "           [-0.0093, -0.0000, -0.0000,  0.0160,  0.0120],\n",
       "           [-0.0091,  0.0103, -0.0086, -0.0000,  0.0215],\n",
       "           [ 0.0198,  0.0000,  0.0174,  0.0121, -0.0240]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000, -0.0200,  0.0245, -0.0109, -0.0000],\n",
       "           [-0.0000,  0.0122,  0.0203,  0.0215,  0.0000],\n",
       "           [-0.0212, -0.0218, -0.0000,  0.0000, -0.0149],\n",
       "           [ 0.0172, -0.0224, -0.0190, -0.0000,  0.0082],\n",
       "           [-0.0212,  0.0140, -0.0000, -0.0251,  0.0000]],\n",
       " \n",
       "          [[-0.0106,  0.0137,  0.0092, -0.0175, -0.0000],\n",
       "           [-0.0000, -0.0079, -0.0124,  0.0100, -0.0248],\n",
       "           [-0.0162, -0.0161,  0.0000,  0.0126, -0.0177],\n",
       "           [ 0.0222,  0.0080, -0.0249,  0.0117, -0.0098],\n",
       "           [-0.0000, -0.0242,  0.0067, -0.0163,  0.0160]],\n",
       " \n",
       "          [[ 0.0000, -0.0223, -0.0184, -0.0000,  0.0163],\n",
       "           [-0.0170,  0.0067, -0.0000, -0.0193,  0.0249],\n",
       "           [-0.0198, -0.0097,  0.0181,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.0175,  0.0069,  0.0148, -0.0246],\n",
       "           [-0.0064,  0.0238, -0.0000,  0.0218,  0.0062]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0000, -0.0206,  0.0074,  0.0085, -0.0174],\n",
       "           [ 0.0000,  0.0116,  0.0147, -0.0000, -0.0174],\n",
       "           [ 0.0122,  0.0000,  0.0152, -0.0000,  0.0143],\n",
       "           [ 0.0000,  0.0177,  0.0216, -0.0164,  0.0124],\n",
       "           [ 0.0000,  0.0226, -0.0225,  0.0157,  0.0155]],\n",
       " \n",
       "          [[ 0.0102, -0.0104, -0.0193,  0.0251, -0.0139],\n",
       "           [ 0.0087, -0.0000, -0.0240, -0.0000,  0.0218],\n",
       "           [ 0.0088,  0.0000,  0.0151,  0.0000,  0.0060],\n",
       "           [-0.0189,  0.0225, -0.0114,  0.0199, -0.0079],\n",
       "           [-0.0211, -0.0000,  0.0187,  0.0168,  0.0000]],\n",
       " \n",
       "          [[-0.0145,  0.0000,  0.0182,  0.0195,  0.0000],\n",
       "           [-0.0206, -0.0000, -0.0225,  0.0260,  0.0113],\n",
       "           [-0.0000,  0.0192,  0.0206, -0.0245, -0.0204],\n",
       "           [-0.0193,  0.0235,  0.0213, -0.0195, -0.0094],\n",
       "           [-0.0184, -0.0194, -0.0137,  0.0148, -0.0091]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0093, -0.0209,  0.0000,  0.0086,  0.0104],\n",
       "           [ 0.0000,  0.0168,  0.0171,  0.0063,  0.0059],\n",
       "           [ 0.0181,  0.0000, -0.0000,  0.0069,  0.0253],\n",
       "           [ 0.0236,  0.0179,  0.0205, -0.0111,  0.0089],\n",
       "           [ 0.0000, -0.0210,  0.0230, -0.0210,  0.0000]],\n",
       " \n",
       "          [[ 0.0075,  0.0000, -0.0100,  0.0084, -0.0261],\n",
       "           [ 0.0148,  0.0000,  0.0153, -0.0000,  0.0000],\n",
       "           [ 0.0139,  0.0113, -0.0146,  0.0141,  0.0136],\n",
       "           [ 0.0000, -0.0238, -0.0073,  0.0063, -0.0101],\n",
       "           [ 0.0000, -0.0224,  0.0090,  0.0108,  0.0113]],\n",
       " \n",
       "          [[ 0.0189,  0.0245, -0.0247, -0.0000, -0.0000],\n",
       "           [ 0.0203,  0.0225,  0.0120, -0.0127,  0.0079],\n",
       "           [ 0.0249, -0.0125, -0.0121,  0.0138, -0.0000],\n",
       "           [ 0.0219, -0.0113,  0.0000, -0.0124, -0.0226],\n",
       "           [ 0.0231,  0.0113, -0.0246, -0.0000,  0.0187]]],\n",
       " \n",
       " \n",
       "         [[[-0.0186,  0.0216,  0.0191,  0.0233,  0.0122],\n",
       "           [-0.0139, -0.0181,  0.0094,  0.0208, -0.0000],\n",
       "           [ 0.0182, -0.0167, -0.0250, -0.0099, -0.0000],\n",
       "           [-0.0203, -0.0191,  0.0104, -0.0231,  0.0000],\n",
       "           [-0.0000, -0.0000, -0.0231, -0.0000,  0.0132]],\n",
       " \n",
       "          [[ 0.0000,  0.0092,  0.0180,  0.0110,  0.0138],\n",
       "           [ 0.0123, -0.0000,  0.0106,  0.0000,  0.0000],\n",
       "           [-0.0000, -0.0125,  0.0191, -0.0100,  0.0000],\n",
       "           [-0.0103,  0.0112,  0.0091, -0.0000,  0.0142],\n",
       "           [-0.0112, -0.0000,  0.0144,  0.0000, -0.0178]],\n",
       " \n",
       "          [[-0.0186,  0.0153,  0.0102, -0.0194,  0.0196],\n",
       "           [ 0.0234,  0.0000,  0.0078,  0.0064, -0.0000],\n",
       "           [-0.0000, -0.0114,  0.0222,  0.0140, -0.0137],\n",
       "           [-0.0000, -0.0000, -0.0167, -0.0213, -0.0000],\n",
       "           [-0.0198, -0.0153,  0.0130, -0.0190,  0.0165]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0061, -0.0253, -0.0243,  0.0240,  0.0068],\n",
       "           [-0.0000,  0.0000,  0.0141, -0.0000,  0.0179],\n",
       "           [ 0.0116,  0.0000, -0.0000,  0.0103,  0.0214],\n",
       "           [ 0.0125, -0.0200, -0.0000, -0.0180, -0.0000],\n",
       "           [-0.0000,  0.0000,  0.0184, -0.0192,  0.0075]],\n",
       " \n",
       "          [[-0.0070, -0.0219, -0.0156,  0.0109, -0.0073],\n",
       "           [ 0.0242,  0.0077,  0.0000, -0.0092, -0.0248],\n",
       "           [ 0.0184,  0.0000, -0.0000, -0.0209, -0.0228],\n",
       "           [ 0.0241, -0.0175,  0.0000, -0.0000,  0.0000],\n",
       "           [ 0.0234,  0.0000, -0.0000,  0.0071,  0.0000]],\n",
       " \n",
       "          [[-0.0091, -0.0241, -0.0246,  0.0194, -0.0216],\n",
       "           [ 0.0116,  0.0000,  0.0237,  0.0000, -0.0198],\n",
       "           [-0.0000,  0.0224, -0.0089, -0.0000,  0.0000],\n",
       "           [-0.0000, -0.0106, -0.0101,  0.0000,  0.0000],\n",
       "           [-0.0110, -0.0155,  0.0157, -0.0149,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.0160, -0.0128, -0.0000,  0.0095, -0.0216],\n",
       "           [ 0.0129,  0.0181,  0.0231, -0.0000,  0.0000],\n",
       "           [-0.0000, -0.0157,  0.0131, -0.0000,  0.0146],\n",
       "           [ 0.0196,  0.0000, -0.0174, -0.0186, -0.0234],\n",
       "           [ 0.0188,  0.0000, -0.0165,  0.0000,  0.0179]],\n",
       " \n",
       "          [[-0.0107, -0.0122,  0.0000,  0.0000,  0.0153],\n",
       "           [ 0.0249,  0.0190, -0.0208,  0.0137,  0.0000],\n",
       "           [ 0.0128,  0.0157, -0.0113,  0.0000,  0.0165],\n",
       "           [-0.0173, -0.0000,  0.0106,  0.0000, -0.0000],\n",
       "           [ 0.0073,  0.0127,  0.0111,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0062, -0.0087,  0.0000, -0.0000, -0.0132],\n",
       "           [-0.0186,  0.0000,  0.0237,  0.0125, -0.0127],\n",
       "           [-0.0158,  0.0229,  0.0072, -0.0130, -0.0175],\n",
       "           [-0.0162,  0.0212,  0.0064, -0.0213, -0.0201],\n",
       "           [-0.0122,  0.0211, -0.0201, -0.0000,  0.0066]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0190,  0.0163,  0.0000, -0.0091,  0.0188],\n",
       "           [ 0.0176, -0.0230, -0.0083,  0.0000, -0.0221],\n",
       "           [ 0.0000,  0.0104, -0.0119, -0.0142, -0.0183],\n",
       "           [ 0.0000, -0.0118,  0.0117, -0.0000, -0.0110],\n",
       "           [ 0.0217,  0.0135, -0.0169,  0.0173,  0.0088]],\n",
       " \n",
       "          [[-0.0252, -0.0000, -0.0258, -0.0234,  0.0067],\n",
       "           [ 0.0167,  0.0217, -0.0000, -0.0213, -0.0149],\n",
       "           [ 0.0294,  0.0080,  0.0213,  0.0112,  0.0230],\n",
       "           [ 0.0245,  0.0000,  0.0181,  0.0000,  0.0133],\n",
       "           [ 0.0138,  0.0187,  0.0213,  0.0252, -0.0177]],\n",
       " \n",
       "          [[-0.0000,  0.0064,  0.0000, -0.0243, -0.0163],\n",
       "           [-0.0084, -0.0000,  0.0254, -0.0178, -0.0126],\n",
       "           [-0.0000,  0.0099,  0.0236, -0.0120,  0.0142],\n",
       "           [ 0.0202,  0.0188, -0.0141,  0.0266, -0.0110],\n",
       "           [ 0.0000,  0.0000, -0.0217, -0.0181, -0.0231]]]]),\n",
       " 'model.features.6': tensor([[[[ 0.0199, -0.0161,  0.0118],\n",
       "           [ 0.0000,  0.0250, -0.0108],\n",
       "           [-0.0129, -0.0000, -0.0157]],\n",
       " \n",
       "          [[ 0.0108,  0.0210, -0.0176],\n",
       "           [ 0.0000, -0.0059,  0.0091],\n",
       "           [ 0.0179, -0.0238,  0.0110]],\n",
       " \n",
       "          [[-0.0069,  0.0137,  0.0229],\n",
       "           [ 0.0000, -0.0169, -0.0000],\n",
       "           [ 0.0124,  0.0196,  0.0149]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0170, -0.0145, -0.0000],\n",
       "           [-0.0093, -0.0000,  0.0172],\n",
       "           [ 0.0244, -0.0210, -0.0213]],\n",
       " \n",
       "          [[-0.0078, -0.0192, -0.0000],\n",
       "           [ 0.0000,  0.0079,  0.0123],\n",
       "           [ 0.0147, -0.0204, -0.0000]],\n",
       " \n",
       "          [[ 0.0196, -0.0059,  0.0063],\n",
       "           [ 0.0115, -0.0000,  0.0226],\n",
       "           [-0.0133,  0.0158,  0.0080]]],\n",
       " \n",
       " \n",
       "         [[[-0.0067,  0.0238,  0.0000],\n",
       "           [ 0.0231, -0.0000, -0.0110],\n",
       "           [-0.0077, -0.0077,  0.0180]],\n",
       " \n",
       "          [[ 0.0092,  0.0244, -0.0155],\n",
       "           [-0.0000,  0.0108, -0.0000],\n",
       "           [-0.0153, -0.0000,  0.0151]],\n",
       " \n",
       "          [[-0.0154,  0.0000, -0.0078],\n",
       "           [-0.0125,  0.0070, -0.0000],\n",
       "           [ 0.0139,  0.0207, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0067, -0.0071, -0.0192],\n",
       "           [ 0.0172, -0.0000,  0.0209],\n",
       "           [ 0.0000, -0.0140, -0.0000]],\n",
       " \n",
       "          [[ 0.0071,  0.0243,  0.0205],\n",
       "           [ 0.0077,  0.0138, -0.0000],\n",
       "           [-0.0000,  0.0159, -0.0180]],\n",
       " \n",
       "          [[-0.0214,  0.0209, -0.0092],\n",
       "           [ 0.0107, -0.0000,  0.0115],\n",
       "           [ 0.0196,  0.0225, -0.0070]]],\n",
       " \n",
       " \n",
       "         [[[-0.0137, -0.0229, -0.0000],\n",
       "           [ 0.0106, -0.0254, -0.0138],\n",
       "           [ 0.0166, -0.0138,  0.0000]],\n",
       " \n",
       "          [[ 0.0069,  0.0000, -0.0198],\n",
       "           [-0.0000, -0.0244,  0.0171],\n",
       "           [-0.0078,  0.0182, -0.0093]],\n",
       " \n",
       "          [[-0.0000,  0.0113,  0.0161],\n",
       "           [ 0.0065, -0.0232, -0.0090],\n",
       "           [-0.0194,  0.0000, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0190, -0.0118,  0.0196],\n",
       "           [-0.0000, -0.0114,  0.0079],\n",
       "           [-0.0075,  0.0229,  0.0179]],\n",
       " \n",
       "          [[ 0.0000,  0.0195, -0.0000],\n",
       "           [-0.0167,  0.0126, -0.0192],\n",
       "           [ 0.0227,  0.0000,  0.0196]],\n",
       " \n",
       "          [[ 0.0154,  0.0147,  0.0235],\n",
       "           [-0.0131, -0.0146,  0.0000],\n",
       "           [ 0.0000,  0.0222,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0157,  0.0000,  0.0226],\n",
       "           [ 0.0127,  0.0000,  0.0204],\n",
       "           [ 0.0074,  0.0217,  0.0000]],\n",
       " \n",
       "          [[-0.0120,  0.0070,  0.0089],\n",
       "           [ 0.0000,  0.0152,  0.0222],\n",
       "           [-0.0144,  0.0000, -0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0161, -0.0000],\n",
       "           [-0.0086, -0.0089,  0.0000],\n",
       "           [ 0.0187, -0.0000,  0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0072, -0.0131, -0.0232],\n",
       "           [ 0.0106, -0.0117,  0.0000],\n",
       "           [-0.0095,  0.0152, -0.0000]],\n",
       " \n",
       "          [[ 0.0164,  0.0097,  0.0151],\n",
       "           [ 0.0000,  0.0225, -0.0148],\n",
       "           [ 0.0079,  0.0000,  0.0142]],\n",
       " \n",
       "          [[ 0.0000, -0.0000, -0.0077],\n",
       "           [-0.0146,  0.0119,  0.0142],\n",
       "           [-0.0199, -0.0000, -0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0250,  0.0109,  0.0144],\n",
       "           [-0.0175, -0.0063, -0.0090],\n",
       "           [-0.0204, -0.0086, -0.0152]],\n",
       " \n",
       "          [[ 0.0000, -0.0143,  0.0147],\n",
       "           [ 0.0244, -0.0177,  0.0220],\n",
       "           [ 0.0161,  0.0079, -0.0000]],\n",
       " \n",
       "          [[ 0.0139, -0.0188, -0.0153],\n",
       "           [ 0.0230, -0.0000, -0.0114],\n",
       "           [ 0.0000,  0.0222, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0069, -0.0217,  0.0062],\n",
       "           [-0.0126, -0.0129,  0.0124],\n",
       "           [ 0.0202, -0.0145,  0.0157]],\n",
       " \n",
       "          [[ 0.0105,  0.0103, -0.0178],\n",
       "           [-0.0130,  0.0261,  0.0000],\n",
       "           [-0.0209,  0.0149,  0.0000]],\n",
       " \n",
       "          [[ 0.0174, -0.0156, -0.0162],\n",
       "           [-0.0089,  0.0213,  0.0207],\n",
       "           [-0.0144,  0.0000, -0.0180]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0138, -0.0197,  0.0000],\n",
       "           [-0.0000,  0.0000, -0.0000],\n",
       "           [-0.0113, -0.0182, -0.0119]],\n",
       " \n",
       "          [[-0.0066,  0.0167, -0.0000],\n",
       "           [-0.0088, -0.0073,  0.0000],\n",
       "           [-0.0106, -0.0144,  0.0060]],\n",
       " \n",
       "          [[-0.0206, -0.0090, -0.0107],\n",
       "           [ 0.0204,  0.0126,  0.0063],\n",
       "           [ 0.0209, -0.0094,  0.0091]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0185,  0.0076,  0.0000],\n",
       "           [-0.0149, -0.0000,  0.0173],\n",
       "           [ 0.0226,  0.0123,  0.0240]],\n",
       " \n",
       "          [[-0.0222, -0.0075,  0.0096],\n",
       "           [-0.0128,  0.0000,  0.0201],\n",
       "           [-0.0072, -0.0000,  0.0178]],\n",
       " \n",
       "          [[ 0.0000,  0.0176, -0.0000],\n",
       "           [-0.0182,  0.0067,  0.0237],\n",
       "           [ 0.0145, -0.0126,  0.0105]]]]),\n",
       " 'model.features.8': tensor([[[[-0.0000, -0.0145, -0.0093],\n",
       "           [ 0.0109,  0.0156,  0.0000],\n",
       "           [-0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "          [[ 0.0137,  0.0101, -0.0130],\n",
       "           [ 0.0134, -0.0165, -0.0000],\n",
       "           [ 0.0068,  0.0132, -0.0000]],\n",
       " \n",
       "          [[-0.0165,  0.0000,  0.0117],\n",
       "           [ 0.0116,  0.0000, -0.0000],\n",
       "           [-0.0152,  0.0126, -0.0153]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0100,  0.0064, -0.0102],\n",
       "           [ 0.0154, -0.0148,  0.0090],\n",
       "           [-0.0149,  0.0128, -0.0160]],\n",
       " \n",
       "          [[ 0.0110,  0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0063, -0.0082],\n",
       "           [ 0.0000,  0.0118,  0.0079]],\n",
       " \n",
       "          [[-0.0128,  0.0000, -0.0160],\n",
       "           [ 0.0000, -0.0134, -0.0131],\n",
       "           [-0.0000, -0.0121, -0.0148]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0147, -0.0095,  0.0214],\n",
       "           [ 0.0117, -0.0000,  0.0187],\n",
       "           [-0.0000, -0.0069,  0.0000]],\n",
       " \n",
       "          [[ 0.0140,  0.0000,  0.0075],\n",
       "           [ 0.0000, -0.0094, -0.0129],\n",
       "           [-0.0101,  0.0168, -0.0076]],\n",
       " \n",
       "          [[ 0.0113, -0.0121, -0.0106],\n",
       "           [-0.0117,  0.0000,  0.0000],\n",
       "           [-0.0141, -0.0129, -0.0074]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000, -0.0099, -0.0069],\n",
       "           [-0.0090,  0.0150, -0.0143],\n",
       "           [ 0.0153, -0.0093, -0.0000]],\n",
       " \n",
       "          [[ 0.0068, -0.0155,  0.0153],\n",
       "           [-0.0143,  0.0113, -0.0000],\n",
       "           [ 0.0111,  0.0000,  0.0235]],\n",
       " \n",
       "          [[-0.0000, -0.0000, -0.0000],\n",
       "           [-0.0086, -0.0101,  0.0000],\n",
       "           [ 0.0000,  0.0108, -0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.0081, -0.0065,  0.0130],\n",
       "           [ 0.0069,  0.0106,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.0080,  0.0153,  0.0115],\n",
       "           [ 0.0099, -0.0130, -0.0104],\n",
       "           [ 0.0000, -0.0153, -0.0171]],\n",
       " \n",
       "          [[ 0.0099,  0.0000, -0.0099],\n",
       "           [-0.0105, -0.0063,  0.0078],\n",
       "           [ 0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0102, -0.0076, -0.0000],\n",
       "           [-0.0068,  0.0000, -0.0121],\n",
       "           [ 0.0000, -0.0110,  0.0000]],\n",
       " \n",
       "          [[ 0.0097,  0.0117, -0.0162],\n",
       "           [ 0.0000,  0.0000, -0.0150],\n",
       "           [-0.0000, -0.0102,  0.0159]],\n",
       " \n",
       "          [[ 0.0000,  0.0156,  0.0149],\n",
       "           [-0.0000, -0.0079,  0.0204],\n",
       "           [-0.0000,  0.0062, -0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0000,  0.0081,  0.0067],\n",
       "           [ 0.0066,  0.0000,  0.0112],\n",
       "           [ 0.0083,  0.0083, -0.0000]],\n",
       " \n",
       "          [[ 0.0076,  0.0166, -0.0112],\n",
       "           [-0.0000, -0.0086, -0.0164],\n",
       "           [-0.0000,  0.0000, -0.0137]],\n",
       " \n",
       "          [[-0.0097, -0.0101,  0.0117],\n",
       "           [-0.0000, -0.0118, -0.0104],\n",
       "           [ 0.0085,  0.0127,  0.0129]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0000,  0.0068,  0.0109],\n",
       "           [-0.0000, -0.0165,  0.0000],\n",
       "           [ 0.0078, -0.0065, -0.0170]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0068],\n",
       "           [-0.0122,  0.0106,  0.0000],\n",
       "           [ 0.0000,  0.0066,  0.0128]],\n",
       " \n",
       "          [[-0.0000,  0.0112, -0.0152],\n",
       "           [ 0.0000, -0.0113, -0.0000],\n",
       "           [ 0.0155, -0.0137, -0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.0118, -0.0085, -0.0174],\n",
       "           [ 0.0141,  0.0000,  0.0087],\n",
       "           [-0.0086,  0.0177, -0.0071]],\n",
       " \n",
       "          [[-0.0130,  0.0112, -0.0167],\n",
       "           [-0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000, -0.0174, -0.0081]],\n",
       " \n",
       "          [[-0.0000, -0.0149, -0.0000],\n",
       "           [-0.0089, -0.0070, -0.0102],\n",
       "           [ 0.0121, -0.0067, -0.0089]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0000,  0.0107, -0.0092],\n",
       "           [ 0.0077, -0.0097, -0.0162],\n",
       "           [ 0.0121, -0.0091,  0.0116]],\n",
       " \n",
       "          [[ 0.0110, -0.0000,  0.0069],\n",
       "           [ 0.0100,  0.0000,  0.0117],\n",
       "           [-0.0157,  0.0000,  0.0142]],\n",
       " \n",
       "          [[ 0.0146,  0.0102, -0.0169],\n",
       "           [ 0.0079, -0.0126, -0.0073],\n",
       "           [ 0.0146,  0.0078,  0.0114]]],\n",
       " \n",
       " \n",
       "         [[[-0.0000,  0.0000, -0.0140],\n",
       "           [-0.0000,  0.0079,  0.0065],\n",
       "           [ 0.0000, -0.0109,  0.0000]],\n",
       " \n",
       "          [[ 0.0000, -0.0000,  0.0000],\n",
       "           [ 0.0124,  0.0000,  0.0091],\n",
       "           [-0.0103, -0.0080,  0.0000]],\n",
       " \n",
       "          [[-0.0092, -0.0000,  0.0103],\n",
       "           [-0.0121,  0.0000,  0.0161],\n",
       "           [-0.0073, -0.0151, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000, -0.0106],\n",
       "           [-0.0000,  0.0110, -0.0112],\n",
       "           [-0.0148,  0.0088,  0.0170]],\n",
       " \n",
       "          [[-0.0000, -0.0000, -0.0104],\n",
       "           [ 0.0000,  0.0074,  0.0106],\n",
       "           [ 0.0000,  0.0073, -0.0137]],\n",
       " \n",
       "          [[-0.0073, -0.0098,  0.0155],\n",
       "           [ 0.0000, -0.0000, -0.0080],\n",
       "           [ 0.0144, -0.0123, -0.0000]]]]),\n",
       " 'model.features.10': tensor([[[[ 0.0093,  0.0000,  0.0150],\n",
       "           [-0.0000, -0.0200,  0.0150],\n",
       "           [ 0.0000, -0.0224, -0.0221]],\n",
       " \n",
       "          [[ 0.0160, -0.0200,  0.0182],\n",
       "           [-0.0114,  0.0114, -0.0134],\n",
       "           [-0.0171, -0.0064,  0.0062]],\n",
       " \n",
       "          [[-0.0108, -0.0101,  0.0153],\n",
       "           [ 0.0121, -0.0103, -0.0228],\n",
       "           [-0.0000, -0.0149, -0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0112,  0.0144,  0.0061],\n",
       "           [-0.0087,  0.0000, -0.0142],\n",
       "           [ 0.0061,  0.0082, -0.0000]],\n",
       " \n",
       "          [[-0.0000, -0.0110,  0.0168],\n",
       "           [-0.0103, -0.0186,  0.0000],\n",
       "           [-0.0173, -0.0132, -0.0000]],\n",
       " \n",
       "          [[-0.0133,  0.0158,  0.0000],\n",
       "           [ 0.0142, -0.0124, -0.0161],\n",
       "           [-0.0158,  0.0137, -0.0225]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000, -0.0092, -0.0000],\n",
       "           [ 0.0113,  0.0000,  0.0098],\n",
       "           [ 0.0222, -0.0083,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0173],\n",
       "           [-0.0073, -0.0202, -0.0164],\n",
       "           [ 0.0076,  0.0132,  0.0072]],\n",
       " \n",
       "          [[-0.0000, -0.0000, -0.0000],\n",
       "           [ 0.0073,  0.0148, -0.0000],\n",
       "           [-0.0000,  0.0000, -0.0089]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0103,  0.0146,  0.0000],\n",
       "           [ 0.0097,  0.0105, -0.0000],\n",
       "           [ 0.0206,  0.0000, -0.0175]],\n",
       " \n",
       "          [[-0.0149,  0.0100,  0.0123],\n",
       "           [-0.0000,  0.0000,  0.0114],\n",
       "           [ 0.0000, -0.0000,  0.0068]],\n",
       " \n",
       "          [[-0.0182,  0.0099,  0.0093],\n",
       "           [-0.0159, -0.0000,  0.0085],\n",
       "           [-0.0000,  0.0145, -0.0060]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0067, -0.0095, -0.0000],\n",
       "           [-0.0204,  0.0000, -0.0113],\n",
       "           [ 0.0085, -0.0000, -0.0000]],\n",
       " \n",
       "          [[ 0.0107, -0.0235,  0.0175],\n",
       "           [ 0.0125, -0.0185, -0.0192],\n",
       "           [-0.0000, -0.0078, -0.0179]],\n",
       " \n",
       "          [[ 0.0122, -0.0188,  0.0139],\n",
       "           [ 0.0143,  0.0121, -0.0000],\n",
       "           [-0.0155, -0.0138,  0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0000, -0.0127, -0.0000],\n",
       "           [-0.0000, -0.0098,  0.0077],\n",
       "           [-0.0193,  0.0000, -0.0187]],\n",
       " \n",
       "          [[-0.0216, -0.0134, -0.0136],\n",
       "           [-0.0099,  0.0078,  0.0139],\n",
       "           [ 0.0094, -0.0225,  0.0072]],\n",
       " \n",
       "          [[-0.0000, -0.0187, -0.0113],\n",
       "           [ 0.0078, -0.0241, -0.0188],\n",
       "           [-0.0204,  0.0119,  0.0122]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0164,  0.0000,  0.0088],\n",
       "           [-0.0102,  0.0102,  0.0209],\n",
       "           [-0.0082,  0.0141,  0.0104]],\n",
       " \n",
       "          [[-0.0000,  0.0000, -0.0000],\n",
       "           [-0.0000, -0.0188, -0.0156],\n",
       "           [-0.0000, -0.0149, -0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0128,  0.0000],\n",
       "           [ 0.0093, -0.0131, -0.0096],\n",
       "           [ 0.0000, -0.0075,  0.0089]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000, -0.0197, -0.0000],\n",
       "           [-0.0000, -0.0163, -0.0000],\n",
       "           [-0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.0069, -0.0000,  0.0171],\n",
       "           [-0.0074, -0.0090,  0.0185],\n",
       "           [ 0.0116,  0.0164, -0.0138]],\n",
       " \n",
       "          [[ 0.0000, -0.0000, -0.0080],\n",
       "           [-0.0202,  0.0158,  0.0167],\n",
       "           [ 0.0124,  0.0154, -0.0157]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0192,  0.0191,  0.0101],\n",
       "           [ 0.0000,  0.0076,  0.0212],\n",
       "           [-0.0000, -0.0126, -0.0000]],\n",
       " \n",
       "          [[-0.0079, -0.0000,  0.0188],\n",
       "           [ 0.0000,  0.0200,  0.0072],\n",
       "           [-0.0083,  0.0000,  0.0139]],\n",
       " \n",
       "          [[-0.0124,  0.0197, -0.0000],\n",
       "           [-0.0000, -0.0154,  0.0124],\n",
       "           [-0.0123,  0.0000,  0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0078,  0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0150, -0.0196],\n",
       "           [-0.0170,  0.0165, -0.0195]],\n",
       " \n",
       "          [[ 0.0062, -0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0167,  0.0190],\n",
       "           [ 0.0157, -0.0094,  0.0102]],\n",
       " \n",
       "          [[ 0.0097,  0.0000,  0.0155],\n",
       "           [ 0.0118,  0.0172,  0.0000],\n",
       "           [ 0.0093,  0.0127, -0.0183]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0127, -0.0000,  0.0074],\n",
       "           [-0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000,  0.0144, -0.0185]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0100],\n",
       "           [-0.0000, -0.0000, -0.0000],\n",
       "           [-0.0068, -0.0000,  0.0193]],\n",
       " \n",
       "          [[ 0.0107,  0.0119, -0.0212],\n",
       "           [ 0.0156, -0.0179, -0.0000],\n",
       "           [-0.0080, -0.0224,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0182,  0.0114, -0.0186],\n",
       "           [-0.0000, -0.0191,  0.0158],\n",
       "           [ 0.0000,  0.0118, -0.0129]],\n",
       " \n",
       "          [[-0.0187,  0.0000, -0.0177],\n",
       "           [ 0.0000,  0.0068,  0.0180],\n",
       "           [-0.0126, -0.0202,  0.0134]],\n",
       " \n",
       "          [[-0.0000,  0.0145,  0.0000],\n",
       "           [-0.0159,  0.0118,  0.0000],\n",
       "           [-0.0134, -0.0161, -0.0000]]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpruned_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for global unstructured []\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for global unstructured\", accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot accuracy vs. pruning percentage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpruning_percentages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPruning Percentage (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3829\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3821\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3828\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Masters/NNProject/venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy vs. pruning percentage\n",
    "plt.plot([p * 100 for p in pruning_percentages], accuracies, marker='o')\n",
    "plt.xlabel('Pruning Percentage (%)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy vs. Pruning Percentage')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
